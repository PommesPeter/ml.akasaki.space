<!DOCTYPE html>
<html lang="zh-CN">
  <head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width,initial-scale=1">
    <title>Dynamic Neural Networks: A Survey | 工具箱的深度学习记事簿</title>
    <meta name="generator" content="VuePress 1.8.2">
    <link rel="icon" href="/statics/logo.svg">
    <meta name="description" content="这里包含了我从入门到依然在入门的过程中接触到的大部分知识。翻翻目录，也许能找到有用的">
    <meta name="keywords" content="Akasaki,Deep learning,Machine learning,工具箱,工具箱的深度学习记事簿,Akasaki的深度学习记事簿">
    
    <link rel="preload" href="/assets/css/0.styles.71478383.css" as="style"><link rel="preload" href="/assets/js/app.579bc41f.js" as="script"><link rel="preload" href="/assets/js/2.024d806c.js" as="script"><link rel="preload" href="/assets/js/3.736cdea2.js" as="script"><link rel="prefetch" href="/assets/js/10.4c442f97.js"><link rel="prefetch" href="/assets/js/11.5da9b736.js"><link rel="prefetch" href="/assets/js/12.f101e6ce.js"><link rel="prefetch" href="/assets/js/13.f58a5ded.js"><link rel="prefetch" href="/assets/js/14.269eb641.js"><link rel="prefetch" href="/assets/js/15.40d2c0cf.js"><link rel="prefetch" href="/assets/js/16.882eb06c.js"><link rel="prefetch" href="/assets/js/17.4851e27a.js"><link rel="prefetch" href="/assets/js/18.8b83e144.js"><link rel="prefetch" href="/assets/js/19.c8730111.js"><link rel="prefetch" href="/assets/js/20.19d1cbe1.js"><link rel="prefetch" href="/assets/js/21.a79aa63f.js"><link rel="prefetch" href="/assets/js/22.7b8ded85.js"><link rel="prefetch" href="/assets/js/23.a7b93b05.js"><link rel="prefetch" href="/assets/js/24.a20d4bb7.js"><link rel="prefetch" href="/assets/js/25.4deeaec9.js"><link rel="prefetch" href="/assets/js/26.631fe133.js"><link rel="prefetch" href="/assets/js/27.ba05f0b5.js"><link rel="prefetch" href="/assets/js/28.ea88ddc3.js"><link rel="prefetch" href="/assets/js/29.54076f26.js"><link rel="prefetch" href="/assets/js/30.1bb315fb.js"><link rel="prefetch" href="/assets/js/31.0ac58d90.js"><link rel="prefetch" href="/assets/js/32.947e4e92.js"><link rel="prefetch" href="/assets/js/33.436c3bb6.js"><link rel="prefetch" href="/assets/js/34.7da1a08f.js"><link rel="prefetch" href="/assets/js/35.e1805606.js"><link rel="prefetch" href="/assets/js/36.14a21cbe.js"><link rel="prefetch" href="/assets/js/37.9d95c874.js"><link rel="prefetch" href="/assets/js/38.ed00ea11.js"><link rel="prefetch" href="/assets/js/39.8c9ed3d7.js"><link rel="prefetch" href="/assets/js/4.99c371bb.js"><link rel="prefetch" href="/assets/js/40.3acf80a1.js"><link rel="prefetch" href="/assets/js/41.68ec6bb9.js"><link rel="prefetch" href="/assets/js/42.8d291817.js"><link rel="prefetch" href="/assets/js/43.04b88cf3.js"><link rel="prefetch" href="/assets/js/44.d3d09b19.js"><link rel="prefetch" href="/assets/js/45.bdc277c6.js"><link rel="prefetch" href="/assets/js/46.1c9a3c14.js"><link rel="prefetch" href="/assets/js/47.475cdb5e.js"><link rel="prefetch" href="/assets/js/48.c5f2409a.js"><link rel="prefetch" href="/assets/js/49.1d13c570.js"><link rel="prefetch" href="/assets/js/5.a24d6838.js"><link rel="prefetch" href="/assets/js/50.b4bea273.js"><link rel="prefetch" href="/assets/js/51.b2425485.js"><link rel="prefetch" href="/assets/js/52.5b562e31.js"><link rel="prefetch" href="/assets/js/53.9db7815d.js"><link rel="prefetch" href="/assets/js/54.ed61c281.js"><link rel="prefetch" href="/assets/js/55.40015417.js"><link rel="prefetch" href="/assets/js/56.d3c74b1f.js"><link rel="prefetch" href="/assets/js/57.2c302e4a.js"><link rel="prefetch" href="/assets/js/58.a20caa6b.js"><link rel="prefetch" href="/assets/js/59.ca9c2a02.js"><link rel="prefetch" href="/assets/js/6.813c1476.js"><link rel="prefetch" href="/assets/js/60.c5c2e28f.js"><link rel="prefetch" href="/assets/js/61.634fe042.js"><link rel="prefetch" href="/assets/js/62.71d03150.js"><link rel="prefetch" href="/assets/js/63.997741ce.js"><link rel="prefetch" href="/assets/js/64.5486b729.js"><link rel="prefetch" href="/assets/js/65.5bf55bed.js"><link rel="prefetch" href="/assets/js/66.f24d51b5.js"><link rel="prefetch" href="/assets/js/67.531749c3.js"><link rel="prefetch" href="/assets/js/68.52e6d9d2.js"><link rel="prefetch" href="/assets/js/69.a2cae568.js"><link rel="prefetch" href="/assets/js/7.b1278ca1.js"><link rel="prefetch" href="/assets/js/70.e524a3a2.js"><link rel="prefetch" href="/assets/js/71.2aac9633.js"><link rel="prefetch" href="/assets/js/72.8e26c8e0.js"><link rel="prefetch" href="/assets/js/73.a9172e83.js"><link rel="prefetch" href="/assets/js/74.e67ca143.js"><link rel="prefetch" href="/assets/js/8.9d3b4527.js"><link rel="prefetch" href="/assets/js/9.f1f6f0e7.js">
    <link rel="stylesheet" href="/assets/css/0.styles.71478383.css">
  </head>
  <body>
    <div id="app" data-server-rendered="true"><div class="theme-container"><header class="navbar"><div class="sidebar-button"><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" role="img" viewBox="0 0 448 512" class="icon"><path fill="currentColor" d="M436 124H12c-6.627 0-12-5.373-12-12V80c0-6.627 5.373-12 12-12h424c6.627 0 12 5.373 12 12v32c0 6.627-5.373 12-12 12zm0 160H12c-6.627 0-12-5.373-12-12v-32c0-6.627 5.373-12 12-12h424c6.627 0 12 5.373 12 12v32c0 6.627-5.373 12-12 12zm0 160H12c-6.627 0-12-5.373-12-12v-32c0-6.627 5.373-12 12-12h424c6.627 0 12 5.373 12 12v32c0 6.627-5.373 12-12 12z"></path></svg></div> <a href="/" class="home-link router-link-active"><!----> <span class="site-name">工具箱的深度学习记事簿</span></a> <div class="links"><div class="search-box"><input aria-label="Search" autocomplete="off" spellcheck="false" value=""> <!----></div> <nav class="nav-links can-hide"><div class="nav-item"><a href="https://github.com/VisualDust/ml.akasaki.space" target="_blank" rel="noopener noreferrer" class="nav-link external">
  View on Github
  <span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a></div><div class="nav-item"><a href="https://github.com/VisualDust" target="_blank" rel="noopener noreferrer" class="nav-link external">
  工具箱
  <span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a></div> <!----></nav></div></header> <div class="sidebar-mask"></div> <aside class="sidebar"><nav class="nav-links"><div class="nav-item"><a href="https://github.com/VisualDust/ml.akasaki.space" target="_blank" rel="noopener noreferrer" class="nav-link external">
  View on Github
  <span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a></div><div class="nav-item"><a href="https://github.com/VisualDust" target="_blank" rel="noopener noreferrer" class="nav-link external">
  工具箱
  <span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a></div> <!----></nav>  <ul class="sidebar-links"><li><section class="sidebar-group collapsable depth-0"><p class="sidebar-heading"><span>第零章：在开始之前</span> <span class="arrow right"></span></p> <!----></section></li><li><section class="sidebar-group collapsable depth-0"><p class="sidebar-heading"><span>第一章上：HelloWorld</span> <span class="arrow right"></span></p> <!----></section></li><li><section class="sidebar-group collapsable depth-0"><p class="sidebar-heading"><span>第一章下：深度学习基础</span> <span class="arrow right"></span></p> <!----></section></li><li><section class="sidebar-group collapsable depth-0"><p class="sidebar-heading"><span>第二章上：卷积神经网络</span> <span class="arrow right"></span></p> <!----></section></li><li><section class="sidebar-group collapsable depth-0"><p class="sidebar-heading"><span>第二章下：经典卷积神经网络</span> <span class="arrow right"></span></p> <!----></section></li><li><section class="sidebar-group collapsable depth-0"><p class="sidebar-heading"><span>第三章上：谈一些计算机视觉方向</span> <span class="arrow right"></span></p> <!----></section></li><li><section class="sidebar-group collapsable depth-0"><p class="sidebar-heading"><span>第三章下：一些计算机视觉任务</span> <span class="arrow right"></span></p> <!----></section></li><li><section class="sidebar-group collapsable depth-0"><p class="sidebar-heading"><span>附录：永远是你的好朋友</span> <span class="arrow right"></span></p> <!----></section></li><li><section class="sidebar-group collapsable depth-0"><p class="sidebar-heading"><span>第五章：Playground</span> <span class="arrow right"></span></p> <!----></section></li><li><section class="sidebar-group collapsable depth-0"><p class="sidebar-heading"><span>第-1章：TensorFlow编程策略</span> <span class="arrow right"></span></p> <!----></section></li><li><section class="sidebar-group collapsable depth-0"><p class="sidebar-heading"><span>第-2章：数字信号处理（DSP）</span> <span class="arrow right"></span></p> <!----></section></li><li><section class="sidebar-group collapsable depth-0"><p class="sidebar-heading open"><span>魔法部日志（又名论文阅读日志）</span> <span class="arrow down"></span></p> <ul class="sidebar-links sidebar-group-items"><li><a href="/unlimited-paper-works/[1]The-Devil-is-in-the-Decoder-Classification-Regression-and-GANs.html" class="sidebar-link">The Devil is in the Decoder: Classification, Regression and GANs</a></li><li><a href="/unlimited-paper-works/[2]Threat-of-Adversarial-Attacks-on-Deep-Learning-in-Computer-Vision-A-Survey.html" class="sidebar-link">Threat of Adversarial Attacks on Deep Learning in Computer Vision: A Survey</a></li><li><a href="/unlimited-paper-works/[3]Progressive-Semantic-Segmentation.html" class="sidebar-link">Progressive Semantic Segmentation</a></li><li><a href="/unlimited-paper-works/[4]Decoders-Matter-for-Semantic-Segmentation-Data-Dependent-Decoding-Enables-Flexible-Feature-Aggregation.html" class="sidebar-link">Decoders Matter for Semantic Segmentation: Data-Dependent Decoding Enables Flexible Feature Aggregation</a></li><li><a href="/unlimited-paper-works/[5]HLA-Face-Joint-High-Low-Adaptation-for-Low-Light-Face-Detection.html" class="sidebar-link">HLA-Face Joint High-Low Adaptation for Low Light Face Detection</a></li><li><a href="/unlimited-paper-works/[6]DeepLab-Semantic-Image-Segmentation-with-Deep-Convolutional-Nets-Atrous-Convolution-and-Fully-Connected-CRFs.html" class="sidebar-link">DeepLab: Semantic Image Segmentation with Deep Convolutional Nets, Atrous Convolution, and Fully Connected CRFs</a></li><li><a href="/unlimited-paper-works/[7]Rethinking-Atrous-Convolution-for-Semantic-Image-Segmentation.html" class="sidebar-link">Rethinking Atrous Convolution for Semantic Image Segmentation</a></li><li><a href="/unlimited-paper-works/[8]Cross-Dataset-Collaborative-Learning-for-Semantic-Segmentation.html" class="sidebar-link">Cross-Dataset Collaborative Learning for Semantic Segmentation</a></li><li><a href="/unlimited-paper-works/[9]Dynamic-Neural-Networks-A-Survey.html" class="active sidebar-link">Dynamic Neural Networks: A Survey</a><ul class="sidebar-sub-headers"><li class="sidebar-sub-header"><a href="/unlimited-paper-works/[9]Dynamic-Neural-Networks-A-Survey.html#介绍-introduction" class="sidebar-link">介绍（Introduction）</a></li><li class="sidebar-sub-header"><a href="/unlimited-paper-works/[9]Dynamic-Neural-Networks-A-Survey.html#样本自适应的动态神经网络-instance-wise-dynamic-networks" class="sidebar-link">样本自适应的动态神经网络（Instance-wise dynamic networks）</a><ul class="sidebar-sub-headers"><li class="sidebar-sub-header"><a href="/unlimited-paper-works/[9]Dynamic-Neural-Networks-A-Survey.html#动态结构-dynamic-architecture" class="sidebar-link">动态结构（Dynamic Architecture）</a></li><li class="sidebar-sub-header"><a href="/unlimited-paper-works/[9]Dynamic-Neural-Networks-A-Survey.html#动态参数-dynamic-parameters" class="sidebar-link">动态参数（Dynamic Parameters）</a></li></ul></li><li class="sidebar-sub-header"><a href="/unlimited-paper-works/[9]Dynamic-Neural-Networks-A-Survey.html#空间自适应的动态神经网络-spatial-wise-dynamic-networks" class="sidebar-link">空间自适应的动态神经网络（Spatial-wise dynamic networks）</a></li><li class="sidebar-sub-header"><a href="/unlimited-paper-works/[9]Dynamic-Neural-Networks-A-Survey.html#时间自适应-temporal-wise-dynamic-network" class="sidebar-link">时间自适应（Temporal-wise dynamic network）</a></li><li class="sidebar-sub-header"><a href="/unlimited-paper-works/[9]Dynamic-Neural-Networks-A-Survey.html#训练和推理" class="sidebar-link">训练和推理</a><ul class="sidebar-sub-headers"><li class="sidebar-sub-header"><a href="/unlimited-paper-works/[9]Dynamic-Neural-Networks-A-Survey.html#推理-inference" class="sidebar-link">推理（Inference）</a></li><li class="sidebar-sub-header"><a href="/unlimited-paper-works/[9]Dynamic-Neural-Networks-A-Survey.html#训练-training" class="sidebar-link">训练（Training）</a></li></ul></li><li class="sidebar-sub-header"><a href="/unlimited-paper-works/[9]Dynamic-Neural-Networks-A-Survey.html#whatever-disscussion" class="sidebar-link">Whatever Disscussion</a></li></ul></li><li><a href="/unlimited-paper-works/[10]Feature-Pyramid-Networks-for-Object-Detection.html" class="sidebar-link">Feature Pyramid Networks for Object Detection</a></li></ul></section></li></ul> </aside> <main class="page"> <div class="theme-default-content content__default"><h1 id="dynamic-neural-networks-a-survey"><a href="#dynamic-neural-networks-a-survey" class="header-anchor">#</a> Dynamic Neural Networks: A Survey</h1> <h3 id="这篇笔记的写作者是visualdust。"><a href="#这篇笔记的写作者是visualdust。" class="header-anchor">#</a> 这篇笔记的写作者是<a href="https://github.com/visualDust" target="_blank" rel="noopener noreferrer">VisualDust<span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a>。</h3> <p>这篇论文是一篇对动态神经网络的综述，原论文<a href="http://arxiv.org/abs/2102.04906" target="_blank" rel="noopener noreferrer">&quot;Dynamic Neural Networks: A Survey&quot;<span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a>主要讲了：</p> <ul><li>概念（Introduction）</li> <li>常见的动态神经网络
<ul><li>Instance-wise Dynamic Networks</li> <li>Spatial-wise Dynamic Networks</li> <li>Temporal-wise Dynamic Network</li></ul></li> <li>推理和训练（Inference and Training）</li> <li>常见应用和代表性工作（Applications）</li></ul> <p>这篇论文对近些年吸引了很多研究者的动态神经网络进行了较为系统的总结概括。</p> <blockquote><p>Dynamic neural network is an emerging research topic in deep learning. Compared to static models which have fixed computational graphs and parameters at the inference stage, dynamic networks can adapt their structures or parameters to different inputs, leading to notable advantages in terms of accuracy, computational efficiency, adaptiveness, etc. In this survey, we comprehensively review this rapidly developing area by dividing dynamic networks into three main categories: 1) instance-wise dynamic models that process each instance with data-dependent architectures or parameters; 2) spatial-wise dynamic networks that conduct adaptive computation with respect to different spatial locations of image data and 3) temporal-wise dynamic models that perform adaptive inference along the temporal dimension for sequential data such as videos and texts. The important research problems of dynamic networks, e.g., architecture design, decision making scheme, optimization technique and applications, are reviewed systematically. Finally, we discuss the open problems in this field together with interesting future research directions.</p></blockquote> <p>动态神经网络近些年的相关研究逐渐变多，比起固定计算图的传统的静态神经网络，动态神经网络能够可以根据输入的具体数据调整它们的结构或是参数，同时在速度和精度方面占有优势。一种比喻是：“在输入较为简单时，动态神经网络可以很快；在输入较为复杂时，动态神经网络可以精度很高”。</p> <p>这篇论文概括地介绍了动态神经网络是如何“动态”的，以及动态带来了怎样的优势。</p> <p>我看了这篇综述受益匪浅，如果有时间的话请阅读<a href="http://arxiv.org/abs/2102.04906" target="_blank" rel="noopener noreferrer">原作<span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a>。本文只是对原作阅读的粗浅笔记。</p> <hr> <h2 id="介绍-introduction"><a href="#介绍-introduction" class="header-anchor">#</a> 介绍（Introduction）</h2> <p>神经网络随着硬件条件的发展逐渐追求更好的效果和更高的性能。作者将视觉领域的神经网络近十年的发展分为这样几个阶段：</p> <ol><li><p>快速发展阶段（Fast developing stage），2012~2015</p> <p>神经网络的设计变得多样化，出现了包括AlexNet、VGG、GoogLeNet在内的一系列代表性网络结构。</p></li> <li><p>发展成熟阶段（Mature stage），2015~2017</p> <p>这个阶段出现了很多至今都起到了很重要的影响的或是依然被大家经常使用的网络结构，例如ResNet、DenseNet等</p></li> <li><p>繁荣发展阶段（Properous stage），2017~Now</p> <p>人们设计了很多多样化的效果优秀的神经网络，并且大量出现了很多新型的神经网络，例如轻量级网络CondenseNet、ShuffleNet，利用自动搜索技术设计的模型NASNet、DARTS，还有这篇论文想要介绍的动态神经网络MSDNet、Block-Drop、Glance and Focus等，以及突然就火起来的Transformer。</p></li></ol> <p>直到现在，CNN structure  has been never more varied。以CNN网络为例，CNN在ImageNet上的分类准确率正在逐渐提高，甚至达到超过人类的分类水平。随着模型的准确率逐渐提升，人们提出了这样的问题：</p> <blockquote><p>如何平衡网络的精度和网络计算开销之间的关系</p></blockquote> <p><img src="/assets/img/image-20210507091720858.9b6217de.png" alt="image-20210507091720858"></p> <p>上面这张图片来自一篇名为<a href="https://arxiv.org/abs/1611.10012" target="_blank" rel="noopener noreferrer">Speed/accuracy trade-offs for modern convolutional object detectors<span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a>的论文，大致描述了一些知名网络结构的<code>开销-精度</code>图。这便是在神经网络成熟之后人们开始关注的问题。人们希望有更加靠近左上角的模型。</p> <p>一个既成事实是，模型的精度往往和模型的宽度和深度相关，当希望达到更高的准确率时，往往会增加模型的深度和宽度，但是这往往会提高神经网络计算的开销，也就是说此时模型会走向模型的右上角。</p> <p><img src="/assets/img/image-20210507093436025.5cb31eb1.jpg" alt="image-20210507093436025"></p> <p>以上面的图片为例，左图是一只的<a href="https://github.com/coronaPolvo" target="_blank" rel="noopener noreferrer">coronaPolvo<span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a>，右图是一只<a href="https://github.com/pommespeter" target="_blank" rel="noopener noreferrer">PommesPeter<span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a>。他们都是我的好朋友，但是我一瞬间即可认出左图，而认出右图则需要几秒钟。这是因为左图画面明亮，且coronaPolvo占据了画面的主体位置；而右侧的PommesPeter并没有占据画面的主体，并且被车的阴影遮挡，光照条件较差难易辨认。</p> <p>有很多研究已经标明了人的大脑在处理信息时的处理过程会受到周围环境以及很多其他上下文的影响。<strong>这便是动态神经网路的基本构想：对于简单的样本，可以轻易认出；对于复杂的样本，可以多花一点时间。或者说，人名为能发现传统CNN的一个缺陷</strong>：</p> <blockquote><p>Most convolutional neural networks recognize all instance with the same architecture.</p></blockquote> <p>所以动态神经网络的能力就是：</p> <blockquote><p>Dynamic neural networks can adapt their architecture for different instances.</p></blockquote> <p>动态神经网络具有以下优势：</p> <ul><li>高效（Efficiency）</li> <li>更强的表达能力（Representation power）</li> <li>更强的适应性（Adaptiveness）</li> <li>兼容性（Compatibility）</li> <li>设计的简单性（Generality）</li> <li>设计的可解释性（Interpretability）</li></ul> <p>下图是这篇论文的整体内容，涵盖非常的广，推荐阅读一下<a href="http://arxiv.org/abs/2102.04906" target="_blank" rel="noopener noreferrer">原文<span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a>。</p> <p><img src="/assets/img/image-20210507090536138.94be29d0.png" alt="image-20210507090536138"></p> <hr> <h2 id="样本自适应的动态神经网络-instance-wise-dynamic-networks"><a href="#样本自适应的动态神经网络-instance-wise-dynamic-networks" class="header-anchor">#</a> 样本自适应的动态神经网络（Instance-wise dynamic networks）</h2> <p>为了在简单样本上获得更快的推理速度，以及在复杂样本上获得更好的精度，一个最简单并且天真的思路就是，导入多个网络模型，并且在输入较为复杂时使用更复杂的网络，输入简单时使用简单的网络。这个方法基本行不通，因为网络无法提前知道一个输入到底是复杂的还是简单的。</p> <p>所以，我们需要研究样本自适应的动态神经网络</p> <blockquote><p>Instance-wise dynamic networks can adapt their architectures or parameters to each instance.</p></blockquote> <p>对不同的输入样本，这种网络会动态调节自己的结构或参数。</p> <p>这篇论文将样本自适应的动态网络分为：</p> <ul><li>动态结构（Dynamic architecture）
<ul><li>动态深度（Dynamic Depth）
<ul><li>早退机制（Early Existing）</li> <li>跳层机制（Layer Skipping）</li></ul></li> <li>动态宽度（Dynamic Width）
<ul><li>跳过神经元（Skip Neurons）</li> <li>跳过通道（Skip Channels）</li> <li>跳过分支（Skip Branches）</li></ul></li> <li>动态路由（Dynamic Routing）</li></ul></li> <li>动态参数
<ul><li>动态参数加权（Attention on weight）</li> <li>动态卷积核形状（Kernel shape adaptation）</li></ul></li></ul> <hr> <h3 id="动态结构-dynamic-architecture"><a href="#动态结构-dynamic-architecture" class="header-anchor">#</a> 动态结构（Dynamic Architecture）</h3> <h4 id="动态深度-dynamic-depth"><a href="#动态深度-dynamic-depth" class="header-anchor">#</a> 动态深度（Dynamic Depth）</h4> <h5 id="早退机制-early-escape"><a href="#早退机制-early-escape" class="header-anchor">#</a> 早退机制（early escape）</h5> <p>简而言之，动态深度就是网络会根据某种机制判定样本是简单的还算复杂的，对于难一点的样本，网络可以将其一算到底，而对于简单一些的样本，网络计算到中间的时候就可以停止计算了。</p> <p>有两种常见的实现方式：</p> <p><img src="/assets/img/image-20210509114605362.1b92a314.png" alt="image-20210509114605362"></p> <ul><li><p>(a)串联多个模型（Cascading of DNNS，比较早期的工作）</p> <p>在这种方法中，一个网络包含了由浅到深的多个模型：输入通过某个模型后得到输出，经过训练得到的“决定函数”会根据输出的具体情况决定是将特征图直接放入线性分类器还是再次输入到下个模型中继续推理。</p> <p>这种方法有一个问题，很多情况下，特征并不能被复用，或者说不同模型并不能很好地“级联”在一起，例如上图中(a)，若<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>M</mi><mi>o</mi><mi>d</mi><mi>e</mi><msub><mi>l</mi><mn>1</mn></msub></mrow><annotation encoding="application/x-tex">Model_1</annotation></semantics></math></span><span aria-hidden="true" class="katex-html"><span class="base"><span class="strut" style="height:0.84444em;vertical-align:-0.15em;"></span><span class="mord mathnormal" style="margin-right:0.10903em;">M</span><span class="mord mathnormal">o</span><span class="mord mathnormal">d</span><span class="mord mathnormal">e</span><span class="mord"><span class="mord mathnormal" style="margin-right:0.01968em;">l</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.30110799999999993em;"><span style="top:-2.5500000000000003em;margin-left:-0.01968em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">1</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span>是VGG，而<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>M</mi><mi>o</mi><mi>d</mi><mi>e</mi><msub><mi>l</mi><mn>2</mn></msub></mrow><annotation encoding="application/x-tex">Model_2</annotation></semantics></math></span><span aria-hidden="true" class="katex-html"><span class="base"><span class="strut" style="height:0.84444em;vertical-align:-0.15em;"></span><span class="mord mathnormal" style="margin-right:0.10903em;">M</span><span class="mord mathnormal">o</span><span class="mord mathnormal">d</span><span class="mord mathnormal">e</span><span class="mord"><span class="mord mathnormal" style="margin-right:0.01968em;">l</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.30110799999999993em;"><span style="top:-2.5500000000000003em;margin-left:-0.01968em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span>是ResNet，那么很明显这样的设计会导<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>M</mi><mi>o</mi><mi>d</mi><mi>e</mi><msub><mi>l</mi><mn>1</mn></msub></mrow><annotation encoding="application/x-tex">Model_1</annotation></semantics></math></span><span aria-hidden="true" class="katex-html"><span class="base"><span class="strut" style="height:0.84444em;vertical-align:-0.15em;"></span><span class="mord mathnormal" style="margin-right:0.10903em;">M</span><span class="mord mathnormal">o</span><span class="mord mathnormal">d</span><span class="mord mathnormal">e</span><span class="mord"><span class="mord mathnormal" style="margin-right:0.01968em;">l</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.30110799999999993em;"><span style="top:-2.5500000000000003em;margin-left:-0.01968em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">1</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span>的输出并不能能作为<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>M</mi><mi>o</mi><mi>d</mi><mi>e</mi><msub><mi>l</mi><mn>2</mn></msub></mrow><annotation encoding="application/x-tex">Model_2</annotation></semantics></math></span><span aria-hidden="true" class="katex-html"><span class="base"><span class="strut" style="height:0.84444em;vertical-align:-0.15em;"></span><span class="mord mathnormal" style="margin-right:0.10903em;">M</span><span class="mord mathnormal">o</span><span class="mord mathnormal">d</span><span class="mord mathnormal">e</span><span class="mord"><span class="mord mathnormal" style="margin-right:0.01968em;">l</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.30110799999999993em;"><span style="top:-2.5500000000000003em;margin-left:-0.01968em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span>的输入，从而演化成花费两个网络的计算开销，导致额外的性能浪费。</p></li> <li><p>(b)添加中间出口（Network with intermediate classifiers）</p></li></ul> <p>如上图中(b)，这种方法往往通过在一个骨干网络的每个模块之后插入出口实现，即在每个模块的后方加入一个出口，通过设计不同的退出准则来决定模型每经过一个模块的计算之后是否已经不再需要后续网络了，从而实现早退。</p> <p>然而，这种使用早退的方法并不是最优的。有研究标明，如果在模型中添加中间出口，往往会影响模型的分类性能。原因是CNN的更深层输出的feature map往往才具有更多的语义信息，提前退出往往会导致特征提取“并不到位”。</p> <p>比较简单的一种解决方法是使用多尺度的、密集连接的网络架构：</p> <p><img src="/assets/img/image-20210509142111488.558f5948.png" alt="image-20210509142111488"></p> <p>在上图中，较低维度的特征通过<code>concat</code>等操作融合到深层的特征中参与分类，这样不同尺度的特征都能够参与分类。其是否退出的标准为某个分类器的置信度是否达到某个阈值。</p> <p><img src="/assets/img/image-20210509170120613.ccf2585c.png" alt="image-20210509170120613"></p> <p>例如，在上图中，一张猫的照片参与了分类，当网络在某个分类器上的输出（一般是<code>softmax</code>输出）的置信度达到一定值的时候，网络就可以退出了，后面的网络将不被执行。从而实现了动态减小网络的计算量。</p> <p>还有许多具有早退机制的网络，如果我看到了会单独写一节来介绍。</p> <h5 id="跨层连接-skip-connections"><a href="#跨层连接-skip-connections" class="header-anchor">#</a> 跨层连接（skip connections）</h5> <p>早退机制是通过在网络执行的某个阶段退出从而节省计算开销的，而跨层连接的动态神经网络则会执行完整个网络，只是在网络的中间层会出现跨层的连接方式。跨层连接一般被实现在一些具有类似于<code>skip connection</code>或是<code>residual connection</code>的网络结构中。</p> <p><img src="/assets/img/image-20210509171717763.26025a6c.png" alt="image-20210509171717763"></p> <p>跨层连接的一种简单实现方法是在具有跨层连接的网络中加入<code>Gating module</code>。如上图，经过训练的<code>Gating module</code>如果输出为1，则不跳过这一层；</p> <p><img src="/assets/img/image-20210509171732525.27180e73.png" alt="image-20210509171732525"></p> <p>当<code>Gating module</code>的输出为0时，就跳过这一层的计算，即输入直接被当作输出。<code>Gating module</code>一般是包含一个被称为门控函数（Gating function）的计算单元。</p> <p>还有一种实现跳层的方法：</p> <p><img src="/assets/img/image-20210509172345469.e2b0d50a.png" alt="image-20210509172345469"></p> <p>如上图，输入在正式进入主干网络之前会先经过一个被称为<code>Policy Network</code>的网络，这个网络会决定对于当前输入应该跳过哪些层，并通知主干网络这样做。<code>Policy Network</code>的输出一般是一个向量，其中包含了哪些层应该被跳过的信息。</p> <p>还有许多具有跳层机制的网络，如果我看到了会单独写一节来介绍。</p> <h4 id="动态宽度-dynamic-width"><a href="#动态宽度-dynamic-width" class="header-anchor">#</a> 动态宽度（Dynamic Width）</h4> <h5 id="动态通道数-dynamic-channel-pruning-in-cnns"><a href="#动态通道数-dynamic-channel-pruning-in-cnns" class="header-anchor">#</a> 动态通道数（Dynamic channel pruning in CNNs）</h5> <p>动态宽度的动态神经网络，顾名思义，这种网络会根据输入动态调整网络的宽度。一种比较简单的思路是，动态调整通道数量：</p> <p><img src="/assets/img/image-20210509173020737.c5a01922.png" alt="image-20210509173020737"></p> <p>在上图中，也出现了一个被称为<code>Gating module</code>的模块，它根据输入的不同会向卷积层提供一个向量，该向量决定了卷积层跳过输出哪些通道。一种可能的方法是，该向量由0和1组成，卷积层将保留1代表的通道，而取消0代表的通道。</p> <p>另一种可行的方法是使用多个不同宽度的网络对输入进行处理，当某个深度的网络的输出（例如<code>softmax</code>输出）达到某个阈值时，就不再加深网络了。</p> <h5 id="专家子网络加权-mixture-of-experts-moe"><a href="#专家子网络加权-mixture-of-experts-moe" class="header-anchor">#</a> 专家子网络加权（Mixture of Experts，MOE）</h5> <p>这种方法是通过将很多不同的网络的结果进行动态加权来提升网络性能的一种方法。</p> <p><img src="/assets/img/image-20210509174050883.4e3c96cc.png" alt="image-20210509174050883"></p> <p>上图是两种可行的思路：</p> <ul><li>(a)是一种“软加权”，对多个网络的输出进行动态加权，每个子网络都会被执行完，通过调节加权达到更好的性能。</li> <li>(b)是一种“硬加权”，通过一个<code>Gating Module</code>决定某个子网络是否参与决策。如果某个子网络不参与决策，则它根本不会被执行。</li></ul> <p>请注意，这种方法会加大计算量和参数量。在后面的动态参数方法中，会介绍一种和该方法思路很类似的方法。</p> <h5 id="动态全连接层大小-dynamic-width-of-fully-connected-layers"><a href="#动态全连接层大小-dynamic-width-of-fully-connected-layers" class="header-anchor">#</a> 动态全连接层大小（Dynamic width of fully-connected layers）</h5> <p>自如起名，动态修改全连接层的大小，不需要任何额外模块和设计。这里不做详细介绍。</p> <hr> <h4 id="动态路由-dynamic-routing"><a href="#动态路由-dynamic-routing" class="header-anchor">#</a> 动态路由（Dynamic Routing）</h4> <p>上面介绍的动态深度和动态宽度的方法广义上实际上都能视为某种简单的动态路由方法。这里的动态路由单独拿出来，指具有更加复杂的超网络结构（超网络不再是简单地链式结构）的动态路由，这种结构甚至会给不同的样本以不同的计算图。</p> <p><img src="/assets/img/image-20210509114605362.c5c2b982.jpg" alt=""></p> <p>这里有两个可能的设计：</p> <ul><li>(c)是一种树状结构</li> <li>(d)是一种多尺度的动态结构</li></ul> <hr> <h3 id="动态参数-dynamic-parameters"><a href="#动态参数-dynamic-parameters" class="header-anchor">#</a> 动态参数（Dynamic Parameters）</h3> <p>动态参数指的是网络会根据输入的不同使用不同的参数对输入进行运算。可能的动态参数方法有：</p> <p><img src="/assets/img/image-20210510083907081.90313ce3.png" alt="image-20210510083907081"></p> <ul><li>(a)使用一个动态参数加权调节（Parameter Adjustment）模块，根据输入产生一个影响运算参数的参数。</li> <li>(b)使用一个动态参数产生（Parameter Generation）模块，根据输入的不同产生不同的参数比如新的卷积核，对输入进行运算。</li> <li>(c)软注意力（Soft attention）方法</li></ul> <h4 id="动态参数加权-attention-on-weight"><a href="#动态参数加权-attention-on-weight" class="header-anchor">#</a> 动态参数加权（Attention on weight）</h4> <p>动态参数的设计能够提升模型的表达能力。下面举一个简单地例子进行说明：</p> <p><img src="/assets/img/image-20210510084536681.c5e7f2e3.png" alt="image-20210510084536681"></p> <p>在上图中<code>+</code>表示加和，<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>α</mi><mn>1</mn></msub></mrow><annotation encoding="application/x-tex">\alpha_1</annotation></semantics></math></span><span aria-hidden="true" class="katex-html"><span class="base"><span class="strut" style="height:0.58056em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.0037em;">α</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.30110799999999993em;"><span style="top:-2.5500000000000003em;margin-left:-0.0037em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">1</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span>、<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>α</mi><mn>2</mn></msub></mrow><annotation encoding="application/x-tex">\alpha_2</annotation></semantics></math></span><span aria-hidden="true" class="katex-html"><span class="base"><span class="strut" style="height:0.58056em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.0037em;">α</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.30110799999999993em;"><span style="top:-2.5500000000000003em;margin-left:-0.0037em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span>、<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>α</mi><mn>3</mn></msub></mrow><annotation encoding="application/x-tex">\alpha_3</annotation></semantics></math></span><span aria-hidden="true" class="katex-html"><span class="base"><span class="strut" style="height:0.58056em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.0037em;">α</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.30110799999999993em;"><span style="top:-2.5500000000000003em;margin-left:-0.0037em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">3</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span>分别表示一个动态参数调节模块产生的权重。根据输入的不同，这三个提前设定的卷积核通过不同的权重加权形成新的卷积核。上图中的这种操作等效于：</p> <p><img src="/assets/img/image-20210510084826504.4bed211f.png" alt="image-20210510084826504"></p> <p>输入分别与三个不同的卷积核进行运算，并且通过<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>α</mi><mn>1</mn></msub></mrow><annotation encoding="application/x-tex">\alpha_1</annotation></semantics></math></span><span aria-hidden="true" class="katex-html"><span class="base"><span class="strut" style="height:0.58056em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.0037em;">α</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.30110799999999993em;"><span style="top:-2.5500000000000003em;margin-left:-0.0037em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">1</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span>、<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>α</mi><mn>2</mn></msub></mrow><annotation encoding="application/x-tex">\alpha_2</annotation></semantics></math></span><span aria-hidden="true" class="katex-html"><span class="base"><span class="strut" style="height:0.58056em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.0037em;">α</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.30110799999999993em;"><span style="top:-2.5500000000000003em;margin-left:-0.0037em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span>、<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>α</mi><mn>3</mn></msub></mrow><annotation encoding="application/x-tex">\alpha_3</annotation></semantics></math></span><span aria-hidden="true" class="katex-html"><span class="base"><span class="strut" style="height:0.58056em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.0037em;">α</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.30110799999999993em;"><span style="top:-2.5500000000000003em;margin-left:-0.0037em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">3</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span>三个权重加权形成输出。这种设计让人不禁想到在动态结构的设计中出现的专家子网络加权（Mixture of Experts，MOE）方法。不过之前的专家子网络加权方法在这种情形下要卷积三次，而动态参数加权的设计只卷积一次。</p> <p>上面这两种等效的表达可以写为下列等式：</p> <p class="katex-block"><span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><mo stretchy="false">(</mo><munder><mo>∑</mo><mi>n</mi></munder><msub><mi>a</mi><mi>n</mi></msub><msub><mi>w</mi><mi>n</mi></msub><mo stretchy="false">)</mo><mo>⋅</mo><mi>x</mi><mo>=</mo><munder><mo>∑</mo><mi>n</mi></munder><msub><mi>a</mi><mi>n</mi></msub><mo stretchy="false">(</mo><msub><mi>w</mi><mi>n</mi></msub><mo>⋅</mo><mi>x</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">(\sum_{n}a_n w_n)\cdot x = \sum_{n}a_n(w_n\cdot x)
</annotation></semantics></math></span><span aria-hidden="true" class="katex-html"><span class="base"><span class="strut" style="height:2.3000100000000003em;vertical-align:-1.250005em;"></span><span class="mopen">(</span><span class="mop op-limits"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.050005em;"><span style="top:-1.8999949999999999em;margin-left:0em;"><span class="pstrut" style="height:3.05em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">n</span></span></span></span><span style="top:-3.0500049999999996em;"><span class="pstrut" style="height:3.05em;"></span><span><span class="mop op-symbol large-op">∑</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:1.250005em;"><span></span></span></span></span></span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord"><span class="mord mathnormal">a</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.151392em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">n</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.02691em;">w</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.151392em;"><span style="top:-2.5500000000000003em;margin-left:-0.02691em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">n</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">⋅</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:0.43056em;vertical-align:0em;"></span><span class="mord mathnormal">x</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:2.3000100000000003em;vertical-align:-1.250005em;"></span><span class="mop op-limits"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.050005em;"><span style="top:-1.8999949999999999em;margin-left:0em;"><span class="pstrut" style="height:3.05em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">n</span></span></span></span><span style="top:-3.0500049999999996em;"><span class="pstrut" style="height:3.05em;"></span><span><span class="mop op-symbol large-op">∑</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:1.250005em;"><span></span></span></span></span></span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord"><span class="mord mathnormal">a</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.151392em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">n</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mopen">(</span><span class="mord"><span class="mord mathnormal" style="margin-right:0.02691em;">w</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.151392em;"><span style="top:-2.5500000000000003em;margin-left:-0.02691em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">n</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">⋅</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathnormal">x</span><span class="mclose">)</span></span></span></span></span></p> <h4 id="动态卷积核形状-kernel-shape-adaptation"><a href="#动态卷积核形状-kernel-shape-adaptation" class="header-anchor">#</a> 动态卷积核形状（Kernel shape adaptation）</h4> <p>动态卷积核形状的方法能根据输入的不同调节卷积核的形状，以此来获得不同的感受野。比较著名的相关工作是一篇叫做Deformable Convolutional Networks（可形变卷积网络）的论文。</p> <p><img src="/assets/img/image-20210510091338617.d4ff64f4.png" alt="image-20210510091338617"></p> <p>上图是论文Deformable Convolutional Networks中的示意图。</p> <h4 id="动态参数和注意力的关系-dynamic-features-or-dynamic-weights"><a href="#动态参数和注意力的关系-dynamic-features-or-dynamic-weights" class="header-anchor">#</a> 动态参数和注意力的关系（Dynamic features or Dynamic weights）</h4> <p>有一个很好的问题：</p> <blockquote><p>The goal of dynamic parameters is generating dynamic features. So why not rescale features directly with attention?</p></blockquote> <p>注意力机制也是为了动态产生特征的，为什么还要有动态参数的方法呢？例如，在较为出名的transformer中，是通过key和query的相似度对value进行动态调节；还有SENet（<a href="https://arxiv.org/abs/1709.01507" target="_blank" rel="noopener noreferrer">Squeeze-and-excitation networks<span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a>）中，对不同通道进行的动态调节。</p> <p><img src="/assets/img/image-20210510094907451.5e2aa4ec.png" alt="image-20210510094907451"></p> <p>上图是SENet中提及的方法，输入经过正常的卷积运算产生一些channel，同时一个注意力模块（Attention Module）接收输入并产生一个注意力向量，作用于卷积产生的这些channel，使它们被乘以不同的权值。这种方法被称作通道注意力（Channel-wise attention）。</p> <p>所以在某些角度上我们可以说：</p> <p class="katex-block"><span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><mi>D</mi><mi>y</mi><mi>n</mi><mi>a</mi><mi>m</mi><mi>i</mi><mi>c</mi><mi>F</mi><mi>e</mi><mi>a</mi><mi>t</mi><mi>u</mi><mi>r</mi><mi>e</mi><mi>s</mi><mo>=</mo><mi>D</mi><mi>y</mi><mi>n</mi><mi>a</mi><mi>m</mi><mi>i</mi><mi>c</mi><mi>W</mi><mi>e</mi><mi>i</mi><mi>g</mi><mi>h</mi><mi>t</mi></mrow><annotation encoding="application/x-tex">DynamicFeatures = DynamicWeight
</annotation></semantics></math></span><span aria-hidden="true" class="katex-html"><span class="base"><span class="strut" style="height:0.8777699999999999em;vertical-align:-0.19444em;"></span><span class="mord mathnormal" style="margin-right:0.03588em;">Dy</span><span class="mord mathnormal">nami</span><span class="mord mathnormal">c</span><span class="mord mathnormal" style="margin-right:0.13889em;">F</span><span class="mord mathnormal">e</span><span class="mord mathnormal">a</span><span class="mord mathnormal">t</span><span class="mord mathnormal">u</span><span class="mord mathnormal">res</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:0.8888799999999999em;vertical-align:-0.19444em;"></span><span class="mord mathnormal" style="margin-right:0.03588em;">Dy</span><span class="mord mathnormal">nami</span><span class="mord mathnormal">c</span><span class="mord mathnormal" style="margin-right:0.13889em;">W</span><span class="mord mathnormal">e</span><span class="mord mathnormal">i</span><span class="mord mathnormal" style="margin-right:0.03588em;">g</span><span class="mord mathnormal">h</span><span class="mord mathnormal">t</span></span></span></span></span></p> <p>表示为公式就是：</p> <p class="katex-block"><span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><mo stretchy="false">(</mo><mi>x</mi><mo>×</mo><mi>W</mi><mo stretchy="false">)</mo><mo>⊗</mo><mi>α</mi><mo>=</mo><mi>x</mi><mo>×</mo><mo stretchy="false">(</mo><mi>W</mi><mo>⊗</mo><mi>α</mi><mo stretchy="false">)</mo><mspace linebreak="newline"></mspace></mrow><annotation encoding="application/x-tex">(x\times W)\otimes \alpha  =  x\times(W\otimes \alpha)\\
</annotation></semantics></math></span><span aria-hidden="true" class="katex-html"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mopen">(</span><span class="mord mathnormal">x</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">×</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathnormal" style="margin-right:0.13889em;">W</span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">⊗</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:0.43056em;vertical-align:0em;"></span><span class="mord mathnormal" style="margin-right:0.0037em;">α</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:0.66666em;vertical-align:-0.08333em;"></span><span class="mord mathnormal">x</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">×</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mopen">(</span><span class="mord mathnormal" style="margin-right:0.13889em;">W</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">⊗</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathnormal" style="margin-right:0.0037em;">α</span><span class="mclose">)</span></span><span class="mspace newline"></span></span></span></span></p> <p>其中，符号<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo>⊗</mo></mrow><annotation encoding="application/x-tex">\otimes</annotation></semantics></math></span><span aria-hidden="true" class="katex-html"><span class="base"><span class="strut" style="height:0.66666em;vertical-align:-0.08333em;"></span><span class="mord">⊗</span></span></span></span>是克罗内克积，可以查阅相关资料进行了解。</p> <p>上述公式中，等号左侧是动态卷积，先使用<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>W</mi></mrow><annotation encoding="application/x-tex">W</annotation></semantics></math></span><span aria-hidden="true" class="katex-html"><span class="base"><span class="strut" style="height:0.68333em;vertical-align:0em;"></span><span class="mord mathnormal" style="margin-right:0.13889em;">W</span></span></span></span>对输入的<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>x</mi></mrow><annotation encoding="application/x-tex">x</annotation></semantics></math></span><span aria-hidden="true" class="katex-html"><span class="base"><span class="strut" style="height:0.43056em;vertical-align:0em;"></span><span class="mord mathnormal">x</span></span></span></span>完成卷积，再乘上参数<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>α</mi></mrow><annotation encoding="application/x-tex">\alpha</annotation></semantics></math></span><span aria-hidden="true" class="katex-html"><span class="base"><span class="strut" style="height:0.43056em;vertical-align:0em;"></span><span class="mord mathnormal" style="margin-right:0.0037em;">α</span></span></span></span>；等号右侧是动态参数，先使用参数<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>α</mi></mrow><annotation encoding="application/x-tex">\alpha</annotation></semantics></math></span><span aria-hidden="true" class="katex-html"><span class="base"><span class="strut" style="height:0.43056em;vertical-align:0em;"></span><span class="mord mathnormal" style="margin-right:0.0037em;">α</span></span></span></span>影响卷积参数<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>w</mi></mrow><annotation encoding="application/x-tex">w</annotation></semantics></math></span><span aria-hidden="true" class="katex-html"><span class="base"><span class="strut" style="height:0.43056em;vertical-align:0em;"></span><span class="mord mathnormal" style="margin-right:0.02691em;">w</span></span></span></span>，再对输入的<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>x</mi></mrow><annotation encoding="application/x-tex">x</annotation></semantics></math></span><span aria-hidden="true" class="katex-html"><span class="base"><span class="strut" style="height:0.43056em;vertical-align:0em;"></span><span class="mord mathnormal">x</span></span></span></span>进行卷积。它们在数学上是等价的。</p> <hr> <h2 id="空间自适应的动态神经网络-spatial-wise-dynamic-networks"><a href="#空间自适应的动态神经网络-spatial-wise-dynamic-networks" class="header-anchor">#</a> 空间自适应的动态神经网络（Spatial-wise dynamic networks）</h2> <p>传统的卷积神经网络存在一个问题：</p> <blockquote><p>Most conventional networks perform the same computation across different spatial locations of an image.</p></blockquote> <p>对于一张图片，在不同位置包含的信息量可能是不一样的。所以传统网络对图像中每个不同的位置使用相同的运算方法听上去会带来很多冗余的计算量。</p> <p>在这篇论文中，作者将空间自适应方法大致分：</p> <ul><li>像素级（Pixel Level）
<ul><li>动态结构（Dynamic Architecture）</li> <li>动态参数（Dynamic Parameter）</li></ul></li> <li>区域级（Region Level）
<ul><li>动态变换（Dynamic Transformation）</li> <li>硬注意力（Hard Attention）</li></ul></li> <li>分辨率级（Resolution Level）
<ul><li>自适应缩放率（Adaptive Scaling Ratios）</li> <li>多尺度架构（Multi-scalue Architecture）</li></ul></li></ul> <h4 id="像素级自适应-pixel-level"><a href="#像素级自适应-pixel-level" class="header-anchor">#</a> 像素级自适应（Pixel Level）</h4> <p>像素级的一种代表工作如下：</p> <p><img src="/assets/img/image-20210510144149172-1620628910961.5daf4db3.png" alt="image-20210510144149172"></p> <p>在上图中，对于输入，经过一个轻量化的计算得到一个Mask代表哪些位置是重要的。根据生成的Mask进行一个稀疏的卷积，并得到一个稀疏的输出。对于没有被Mask覆盖的区域，可能通过Skip Connection等方式直接跳过运算，直接得到输出。</p> <h4 id="区域级自适应-region-level"><a href="#区域级自适应-region-level" class="header-anchor">#</a> 区域级自适应（Region-Level）</h4> <p>区域级空间自适应方法的一种可行的方法是：</p> <p><img src="/assets/img/image-20210510145118821.2eb7e6b1.png" alt="image-20210510145118821"></p> <p>将图片输入到一个选择器中，选择器选出其中比较重要的一部分（抠出一个小Patch），并只将这一部分输入进网络或运算模块。</p> <p><img src="/assets/img/image-20210510145427441.d50598d9.png" alt="image-20210510145427441"></p> <p>上图是一篇名为<a href="https://arxiv.org/abs/2010.05300" target="_blank" rel="noopener noreferrer">Glance and Focus: a Dynamic Approach to Reducing Spatial Redundancy in Image Classification<span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a>的相关论文中的一个示意图。对于较为简单的样本，在一个小分辨率上直接得到可信的预测；对于复杂的样本，在网络得到置信度很高的输出之前，不断从图片中选择“较为重要”的一部分继续推测。</p> <p>这种方法除了实现了一种注意力机制之外，还实现了网络的早退。</p> <h4 id="分辨率级自适应-resolution-level"><a href="#分辨率级自适应-resolution-level" class="header-anchor">#</a> 分辨率级自适应（Resolution-Level）</h4> <p>分辨率自适应也是一种基于早退的方法，其基本构想是使用递进的网络深度处理不同分辨率的输入。</p> <p><img src="/assets/img/image-20210510150347394.b5563991.png" alt="image-20210510150347394"></p> <p>上图截取自相关工作RANet（<a href="https://arxiv.org/abs/2003.07326" target="_blank" rel="noopener noreferrer">Resolution Adaptive Networks for Efficient Inference<span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a>），对于简单的输入，使用一个很小的子网络，若达到很好的置信度输出，则早退；对于复杂的输入，使用更深的网络进行推断。</p> <p><img src="/assets/img/image-20210510150607144.f7e2fbed.png" alt="image-20210510150607144"></p> <p>上图是RANet的一种推断过程。图中各层<code>Conv Block</code>之间的蓝色箭头实现了特征复用。</p> <h2 id="时间自适应-temporal-wise-dynamic-network"><a href="#时间自适应-temporal-wise-dynamic-network" class="header-anchor">#</a> 时间自适应（Temporal-wise dynamic network）</h2> <p>对于序列的输入，例如视频或文本的输入，可以使用时间自适应的动态网络结构。</p> <p>这篇论文将时间自适应的网络分为：</p> <ul><li>用于处理文本（Text）
<ul><li>动态更新隐藏态（Dynamic Update of Hidden States）</li> <li>时间早退（Temporally Early Exiting）</li> <li>动态跳跃（Dynamic Jumping）</li></ul></li> <li>用于处理视频（Video）
<ul><li>动态循环神经网络（Dynamic RNNs）
<ul><li>动态更新隐藏态（Dynamic Update of Hidden States）</li> <li>时间早退（Temporally Early Exiting）</li> <li>动态跳跃（Dynamic Jumping）</li></ul></li> <li>帧采样（Frame Sampling）</li></ul></li></ul> <p><img src="/assets/img/image-20210510151837557.df90f2b2.png" alt="image-20210510151837557"></p> <p>上图是常见的时间自适应的设计。 其中：</p> <ul><li>(a)中的<code>Agent</code>接受时间戳为<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>t</mi></mrow><annotation encoding="application/x-tex">t</annotation></semantics></math></span><span aria-hidden="true" class="katex-html"><span class="base"><span class="strut" style="height:0.61508em;vertical-align:0em;"></span><span class="mord mathnormal">t</span></span></span></span>的输入<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>x</mi><mi>t</mi></msub></mrow><annotation encoding="application/x-tex">x_t</annotation></semantics></math></span><span aria-hidden="true" class="katex-html"><span class="base"><span class="strut" style="height:0.58056em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathnormal">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.2805559999999999em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">t</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span>，并判断该输入是否重要。若重要，则保留输入进网络并获得当前时间戳输出<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>h</mi><mi>t</mi></msub></mrow><annotation encoding="application/x-tex">h_t</annotation></semantics></math></span><span aria-hidden="true" class="katex-html"><span class="base"><span class="strut" style="height:0.84444em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathnormal">h</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.2805559999999999em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">t</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span>；若不重要，则直接将上一个时间戳<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>h</mi><mrow><mi>t</mi><mo>−</mo><mn>1</mn></mrow></msub></mrow><annotation encoding="application/x-tex">h_{t-1}</annotation></semantics></math></span><span aria-hidden="true" class="katex-html"><span class="base"><span class="strut" style="height:0.902771em;vertical-align:-0.208331em;"></span><span class="mord"><span class="mord mathnormal">h</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.301108em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">t</span><span class="mbin mtight">−</span><span class="mord mtight">1</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.208331em;"><span></span></span></span></span></span></span></span></span></span>作为输出，不进行任何计算。</li> <li>(b)中通过更窄一点的RNN模块进行更新，对于<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>h</mi><mrow><mi>t</mi><mo>−</mo><mn>1</mn></mrow></msub></mrow><annotation encoding="application/x-tex">h_{t-1}</annotation></semantics></math></span><span aria-hidden="true" class="katex-html"><span class="base"><span class="strut" style="height:0.902771em;vertical-align:-0.208331em;"></span><span class="mord"><span class="mord mathnormal">h</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.301108em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">t</span><span class="mbin mtight">−</span><span class="mord mtight">1</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.208331em;"><span></span></span></span></span></span></span></span></span></span>和<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>h</mi><mi>t</mi></msub></mrow><annotation encoding="application/x-tex">h_t</annotation></semantics></math></span><span aria-hidden="true" class="katex-html"><span class="base"><span class="strut" style="height:0.84444em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathnormal">h</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.2805559999999999em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">t</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span>中下半部分灰色的部分也是直接copy的。</li> <li>(d)的基本思路是采样输入中较为重要的位置，具体来说就是通过自适应让<code>RNN</code>模块跳过输入视频中的一部分帧。例如<a href="https://arxiv.org/abs/1804.00931" target="_blank" rel="noopener noreferrer">Dynamic Video Segmentation Network<span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a>。</li></ul> <hr> <h2 id="训练和推理"><a href="#训练和推理" class="header-anchor">#</a> 训练和推理</h2> <h3 id="推理-inference"><a href="#推理-inference" class="header-anchor">#</a> 推理（Inference）</h3> <h4 id="基于置信度-based-on-confidence"><a href="#基于置信度-based-on-confidence" class="header-anchor">#</a> 基于置信度（Based on confidence）</h4> <p><img src="/assets/img/image-20210509170120613.ccf2585c.png" alt="image-20210509170120613"></p> <p>早退机制的常用方法。当网络在某个阶段输出的<code>softmax</code>置信度达到某个阈值时网络就会退出。该方法不需要特殊的运算和设计，仅需要设计一个阈值。</p> <h4 id="基于政策网络-based-on-policy-networks"><a href="#基于政策网络-based-on-policy-networks" class="header-anchor">#</a> 基于政策网络（Based on Policy Networks）</h4> <p><img src="/assets/img/image-20210509172345469.e2b0d50a.png" alt="image-20210509172345469"></p> <p>该方法在跳层实现的动态神经网络中常见。这种<code>Policy Network</code>的设计往往是基于主干网络的结构的，比如其输出的长度可能等于主干网络的<code>Block</code>总数，因此需要专门设计，并不是很通用。</p> <h4 id="基于门控函数-based-on-gating-functions"><a href="#基于门控函数-based-on-gating-functions" class="header-anchor">#</a> 基于门控函数（Based on Gating Functions）</h4> <p><img src="/assets/img/image-20210510204008178.34ef1724.png" alt="image-20210510204008178"></p> <p>这种方法在很多动态神经网络中常见。通常<code>Gating Function</code>的职责可能是：</p> <ul><li>决定哪些网络层被跳过</li> <li>决定哪些专家网络（Expert Network）被执行</li> <li>决定哪些通道（Channel）被忽略</li> <li>决定图像中的哪些区域被采样和卷积的</li></ul> <p><code>Gating Function</code>的计算量通常较小，并且设计上具有即插即用的特点，更加常用。但是这类方法也有一个缺点，就是更长的训练过程。</p> <hr> <h3 id="训练-training"><a href="#训练-training" class="header-anchor">#</a> 训练（Training）</h3> <h4 id="训练目标-objectives"><a href="#训练目标-objectives" class="header-anchor">#</a> 训练目标（Objectives）</h4> <h5 id="早退网络的目标函数-multi-exit-networks"><a href="#早退网络的目标函数-multi-exit-networks" class="header-anchor">#</a> 早退网络的目标函数（Multi-exit Networks）</h5> <p>一种比较普通的思路是：</p> <p class="katex-block"><span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><mi>L</mi><mo stretchy="false">(</mo><mi>y</mi><mo separator="true">,</mo><mi>f</mi><mo stretchy="false">(</mo><mi>x</mi><mo separator="true">;</mo><mi>θ</mi><mo stretchy="false">)</mo><mo stretchy="false">)</mo><mo>=</mo><munder><mo>∑</mo><mi>i</mi></munder><msub><mi>λ</mi><mi>i</mi></msub><mi>C</mi><mi>E</mi><mo stretchy="false">(</mo><mi>y</mi><mo separator="true">,</mo><msub><mi>f</mi><mi>i</mi></msub><mo stretchy="false">(</mo><mi>x</mi><mo separator="true">;</mo><msub><mi>θ</mi><mi>i</mi></msub><mo stretchy="false">)</mo><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">L(y,f(x;\theta)) = \sum_{i}\lambda_i CE(y,f_i(x;\theta_i))
</annotation></semantics></math></span><span aria-hidden="true" class="katex-html"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathnormal">L</span><span class="mopen">(</span><span class="mord mathnormal" style="margin-right:0.03588em;">y</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord mathnormal" style="margin-right:0.10764em;">f</span><span class="mopen">(</span><span class="mord mathnormal">x</span><span class="mpunct">;</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord mathnormal" style="margin-right:0.02778em;">θ</span><span class="mclose">))</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:2.327674em;vertical-align:-1.277669em;"></span><span class="mop op-limits"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.0500050000000003em;"><span style="top:-1.872331em;margin-left:0em;"><span class="pstrut" style="height:3.05em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">i</span></span></span></span><span style="top:-3.050005em;"><span class="pstrut" style="height:3.05em;"></span><span><span class="mop op-symbol large-op">∑</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:1.277669em;"><span></span></span></span></span></span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord"><span class="mord mathnormal">λ</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.31166399999999994em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mord mathnormal" style="margin-right:0.05764em;">CE</span><span class="mopen">(</span><span class="mord mathnormal" style="margin-right:0.03588em;">y</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.10764em;">f</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.31166399999999994em;"><span style="top:-2.5500000000000003em;margin-left:-0.10764em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mopen">(</span><span class="mord mathnormal">x</span><span class="mpunct">;</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.02778em;">θ</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.31166399999999994em;"><span style="top:-2.5500000000000003em;margin-left:-0.02778em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mclose">))</span></span></span></span></span></p> <p>其中，<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>C</mi><mi>E</mi></mrow><annotation encoding="application/x-tex">CE</annotation></semantics></math></span><span aria-hidden="true" class="katex-html"><span class="base"><span class="strut" style="height:0.68333em;vertical-align:0em;"></span><span class="mord mathnormal" style="margin-right:0.05764em;">CE</span></span></span></span>是交叉熵损失函数，<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>λ</mi><mi>i</mi></msub></mrow><annotation encoding="application/x-tex">\lambda_i</annotation></semantics></math></span><span aria-hidden="true" class="katex-html"><span class="base"><span class="strut" style="height:0.84444em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathnormal">λ</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.31166399999999994em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span>是第<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>i</mi></mrow><annotation encoding="application/x-tex">i</annotation></semantics></math></span><span aria-hidden="true" class="katex-html"><span class="base"><span class="strut" style="height:0.65952em;vertical-align:0em;"></span><span class="mord mathnormal">i</span></span></span></span>个退出位置的权值。有的工作中甚至<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>λ</mi><mi>i</mi></msub></mrow><annotation encoding="application/x-tex">\lambda_i</annotation></semantics></math></span><span aria-hidden="true" class="katex-html"><span class="base"><span class="strut" style="height:0.84444em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathnormal">λ</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.31166399999999994em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span>设置恒等于1。也有更加复杂的用于提升多个不同分类器组合训练效果的目标：</p> <p class="katex-block"><span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><msub><mi>L</mi><mi>i</mi></msub><mo>=</mo><mi>α</mi><mi>C</mi><msub><mi>E</mi><mi>i</mi></msub><mo>+</mo><mo stretchy="false">(</mo><mn>1</mn><mo>−</mo><mi>α</mi><mo stretchy="false">)</mo><mi>K</mi><mi>L</mi><msub><mi>D</mi><mi>i</mi></msub><mspace linebreak="newline"></mspace><mi>K</mi><mi>L</mi><msub><mi>D</mi><mi>i</mi></msub><mo>=</mo><mo>−</mo><munder><mo>∑</mo><mrow><mi>c</mi><mo>∈</mo><mi>Y</mi></mrow></munder><msub><mi>p</mi><mi>k</mi></msub><mo stretchy="false">(</mo><mi>c</mi><mi mathvariant="normal">∣</mi><mi>x</mi><mo separator="true">;</mo><mi>θ</mi><mo separator="true">,</mo><mi>T</mi><mo stretchy="false">)</mo><mi>log</mi><mo>⁡</mo><mfrac><mrow><msub><mi>p</mi><mi>i</mi></msub><mo stretchy="false">(</mo><mi>c</mi><mi mathvariant="normal">∣</mi><mi>x</mi><mo separator="true">;</mo><mi>θ</mi><mo separator="true">,</mo><mi>T</mi><mo stretchy="false">)</mo></mrow><mrow><msub><mi>p</mi><mi>k</mi></msub><mo stretchy="false">(</mo><mi>c</mi><mi mathvariant="normal">∣</mi><mi>x</mi><mo separator="true">;</mo><mi>θ</mi><mo separator="true">,</mo><mi>T</mi><mo stretchy="false">)</mo></mrow></mfrac></mrow><annotation encoding="application/x-tex">L_i = \alpha CE_i + (1-\alpha)KLD_i\\
KLD_i = -\sum_{c\in Y}p_k(c|x;\theta ,T)\log\frac{p_i(c|x;\theta ,T)}{p_k(c|x;\theta ,T)}
</annotation></semantics></math></span><span aria-hidden="true" class="katex-html"><span class="base"><span class="strut" style="height:0.83333em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathnormal">L</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.31166399999999994em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:0.83333em;vertical-align:-0.15em;"></span><span class="mord mathnormal" style="margin-right:0.0037em;">α</span><span class="mord mathnormal" style="margin-right:0.07153em;">C</span><span class="mord"><span class="mord mathnormal" style="margin-right:0.05764em;">E</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.31166399999999994em;"><span style="top:-2.5500000000000003em;margin-left:-0.05764em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mopen">(</span><span class="mord">1</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">−</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathnormal" style="margin-right:0.0037em;">α</span><span class="mclose">)</span><span class="mord mathnormal" style="margin-right:0.07153em;">K</span><span class="mord mathnormal">L</span><span class="mord"><span class="mord mathnormal" style="margin-right:0.02778em;">D</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.31166399999999994em;"><span style="top:-2.5500000000000003em;margin-left:-0.02778em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span><span class="mspace newline"></span><span class="base"><span class="strut" style="height:0.83333em;vertical-align:-0.15em;"></span><span class="mord mathnormal" style="margin-right:0.07153em;">K</span><span class="mord mathnormal">L</span><span class="mord"><span class="mord mathnormal" style="margin-right:0.02778em;">D</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.31166399999999994em;"><span style="top:-2.5500000000000003em;margin-left:-0.02778em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:2.7487060000000003em;vertical-align:-1.321706em;"></span><span class="mord">−</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mop op-limits"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.050005em;"><span style="top:-1.8556639999999998em;margin-left:0em;"><span class="pstrut" style="height:3.05em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">c</span><span class="mrel mtight">∈</span><span class="mord mathnormal mtight" style="margin-right:0.22222em;">Y</span></span></span></span><span style="top:-3.0500049999999996em;"><span class="pstrut" style="height:3.05em;"></span><span><span class="mop op-symbol large-op">∑</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:1.321706em;"><span></span></span></span></span></span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord"><span class="mord mathnormal">p</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.33610799999999996em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.03148em;">k</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mopen">(</span><span class="mord mathnormal">c</span><span class="mord">∣</span><span class="mord mathnormal">x</span><span class="mpunct">;</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord mathnormal" style="margin-right:0.02778em;">θ</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord mathnormal" style="margin-right:0.13889em;">T</span><span class="mclose">)</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mop">lo<span style="margin-right:0.01389em;">g</span></span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.427em;"><span style="top:-2.314em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord"><span class="mord mathnormal">p</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.33610799999999996em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.03148em;">k</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mopen">(</span><span class="mord mathnormal">c</span><span class="mord">∣</span><span class="mord mathnormal">x</span><span class="mpunct">;</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord mathnormal" style="margin-right:0.02778em;">θ</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord mathnormal" style="margin-right:0.13889em;">T</span><span class="mclose">)</span></span></span><span style="top:-3.23em;"><span class="pstrut" style="height:3em;"></span><span class="frac-line" style="border-bottom-width:0.04em;"></span></span><span style="top:-3.677em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord"><span class="mord mathnormal">p</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.31166399999999994em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mopen">(</span><span class="mord mathnormal">c</span><span class="mord">∣</span><span class="mord mathnormal">x</span><span class="mpunct">;</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord mathnormal" style="margin-right:0.02778em;">θ</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord mathnormal" style="margin-right:0.13889em;">T</span><span class="mclose">)</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.936em;"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span></span></span></span></span></p> <h5 id="鼓励稀疏度的目标函数-training-objectives-for-encouraging-sparsity"><a href="#鼓励稀疏度的目标函数-training-objectives-for-encouraging-sparsity" class="header-anchor">#</a> 鼓励稀疏度的目标函数（Training Objectives for Encouraging Sparsity）</h5> <p>这类目标函数主要是为了训练跳层或是忽略通道的动态神经网络而设计的。通常情况下，这类网络需要动态调用网络中不同的模块。如果不单独设计新的目标函数，可能会导致网络陷入“尽可能多地调用网络的模块”或是“网络中某个模块一次也没有被调用”。因此很多工作加入了一种称为鼓励稀疏度的目标函数：</p> <p class="katex-block"><span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><mi>L</mi><mo>=</mo><msub><mi>L</mi><mrow><mi>t</mi><mi>a</mi><mi>s</mi><mi>k</mi></mrow></msub><mo>+</mo><msub><mi>L</mi><mrow><mi>s</mi><mi>p</mi><mi>a</mi><mi>r</mi><mi>s</mi><mi>e</mi></mrow></msub></mrow><annotation encoding="application/x-tex">L = L_{task} + L_{sparse}
</annotation></semantics></math></span><span aria-hidden="true" class="katex-html"><span class="base"><span class="strut" style="height:0.68333em;vertical-align:0em;"></span><span class="mord mathnormal">L</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:0.83333em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathnormal">L</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.33610799999999996em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">t</span><span class="mord mathnormal mtight">a</span><span class="mord mathnormal mtight">s</span><span class="mord mathnormal mtight" style="margin-right:0.03148em;">k</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:0.969438em;vertical-align:-0.286108em;"></span><span class="mord"><span class="mord mathnormal">L</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.15139200000000003em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">s</span><span class="mord mathnormal mtight">p</span><span class="mord mathnormal mtight">a</span><span class="mord mathnormal mtight">rse</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.286108em;"><span></span></span></span></span></span></span></span></span></span></span></p> <p>其中，<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>L</mi><mrow><mi>s</mi><mi>p</mi><mi>a</mi><mi>r</mi><mi>s</mi><mi>e</mi></mrow></msub></mrow><annotation encoding="application/x-tex">L_{sparse}</annotation></semantics></math></span><span aria-hidden="true" class="katex-html"><span class="base"><span class="strut" style="height:0.969438em;vertical-align:-0.286108em;"></span><span class="mord"><span class="mord mathnormal">L</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.15139200000000003em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">s</span><span class="mord mathnormal mtight">p</span><span class="mord mathnormal mtight">a</span><span class="mord mathnormal mtight">rse</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.286108em;"><span></span></span></span></span></span></span></span></span></span>可以但不限于表示：</p> <ul><li><p>被激活或是被调用的网络模块的比率。很明显的是，当网络试图“尽可能使用全部模块”时，目标函数的值会增大。</p></li> <li><p>动态调用的模型的复杂度，可以是计算量或是参数量的估算值（FLOPs）。</p></li></ul> <h4 id="优化方法-optimization"><a href="#优化方法-optimization" class="header-anchor">#</a> 优化方法（Optimization）</h4> <h5 id="梯度估计-gradient-estimation"><a href="#梯度估计-gradient-estimation" class="header-anchor">#</a> 梯度估计（Gradient Estimation）</h5> <p>一种可行的方法是使用<code>Gumbel-Softmax</code>对离散变量再参数化，这种方法主要用于优化<code>Gating Function</code>，使其嫩能构成完整的端到端训练系统。坏消息就是这种方法又引入了一个称为<code>Gumbel Noise</code>的随机变量，需要更长的训练过程，并且容易对某些超参数敏感。</p> <h5 id="强化学习-reinforcement-learning"><a href="#强化学习-reinforcement-learning" class="header-anchor">#</a> 强化学习（Reinforcement Learning）</h5> <p>强化学习已经被应用于很多决策任务中。强化学习在动态神经网络中主要用于决定跳层、决定选择哪些Patch以及决定采样哪些帧。</p> <hr> <h2 id="whatever-disscussion"><a href="#whatever-disscussion" class="header-anchor">#</a> Whatever Disscussion</h2> <p><img src="/assets/img/image-20210510215257391.24afcd86.png" alt="image-20210510215257391"></p> <p>动态神经网络已经被应用于很多领域中（如上表）。</p> <p>最后，这篇论文还对一些开放性问题进行了讨论：</p> <ul><li><p>动态神经网络理论（Theories）</p> <p>例如在多出口的网络里，早退机制的判断标准的阈值决策方法是没有很好的理论支撑的。因此，研究具有理论保障的动态决策方法也许可以进一步提升网络性能。</p></li> <li><p>结构设计（Architecture Design）</p> <p>多尺度的动态网络结构设计说明了特别的网络设计对于提升网络的效率是有很大帮助的：在链式的结构中，浅层的feature并不能很好的被用来分类，而使用多尺度的设计就能让分类器达到更好地效果。</p></li> <li><p>在不同任务上的可用性（Applicability on more diverse tasks）</p> <p>当前的很多动态神经网络都是为了分类任务而设计的。虽然很多思想（例如早退和跳层）理论上可以被应用于其他任务，但是大多数现存的动态网络设计并不能像ResNet、DenseNet一样被作为一个比较通用的主干网络，用于很多下游的任务。</p></li> <li><p>理论和实际的差距（Gap between theoretical &amp; practical efficiency）</p> <p>现在的一些动态神经网络受限于计算机硬件或是软件库的设计，并不能达到理论上的最优效果，比如上文中提到的一些空间自适应的设计，当前的图形处理器（GPU）及其运算库的设计对静态神经网络以及整张图片的卷积采样运算优化已经很好了，使用这种设计反而需要单独进行特殊设计。一个重要的议题是如何设计出在当前硬件条件下所带来的运算效率提升。</p></li> <li><p>鲁棒性（Robustness）</p> <p>已经有工作研究了多出口网络在对抗攻击方面的鲁棒性，也有一个很有意思的网络，不但对网络的准确性发起攻击，还对网络的效率发起攻击，这也是一个研究方向。</p></li> <li><p>可解释性（Interpretability）</p> <p>动态神经网络在时间和空间上的硬注意力机制和人的神经系统决策过程也是非常相似的。我们在看一张图片的时候也可能是先粗略地看一眼，然后再单独去看每个小Patch；人在看视频的时候也可能只需要看一些帧就能大概Get到这个视频在讲述什么。</p></li></ul></div> <footer class="page-edit"><!----> <div class="last-updated"><span class="prefix">上次更新时间:</span> <span class="time">5/11/2021, 6:22:54 PM</span></div></footer> <div class="page-nav"><p class="inner"><span class="prev">
      ←
      <a href="/unlimited-paper-works/[8]Cross-Dataset-Collaborative-Learning-for-Semantic-Segmentation.html" class="prev">
        Cross-Dataset Collaborative Learning for Semantic Segmentation
      </a></span> <span class="next"><a href="/unlimited-paper-works/[10]Feature-Pyramid-Networks-for-Object-Detection.html">
        Feature Pyramid Networks for Object Detection
      </a>
      →
    </span></p></div> </main></div><div class="global-ui"></div></div>
    <script src="/assets/js/app.579bc41f.js" defer></script><script src="/assets/js/2.024d806c.js" defer></script><script src="/assets/js/3.736cdea2.js" defer></script>
  </body>
</html>
