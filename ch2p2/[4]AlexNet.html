<!DOCTYPE html>
<html lang="zh-CN">
  <head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width,initial-scale=1">
    <title>AlexNet：更深的卷积神经网络 | 工具箱的深度学习记事簿</title>
    <meta name="generator" content="VuePress 1.8.2">
    <link rel="icon" href="/statics/logo.svg">
    <meta name="description" content="这里包含了我从入门到依然在入门的过程中接触到的大部分知识。翻翻目录，也许能找到有用的">
    <meta name="keywords" content="Akasaki,Deep learning,Machine learning,工具箱,工具箱的深度学习记事簿,Akasaki的深度学习记事簿">
    
    <link rel="preload" href="/assets/css/0.styles.71478383.css" as="style"><link rel="preload" href="/assets/js/app.579bc41f.js" as="script"><link rel="preload" href="/assets/js/2.024d806c.js" as="script"><link rel="preload" href="/assets/js/16.882eb06c.js" as="script"><link rel="prefetch" href="/assets/js/10.4c442f97.js"><link rel="prefetch" href="/assets/js/11.5da9b736.js"><link rel="prefetch" href="/assets/js/12.f101e6ce.js"><link rel="prefetch" href="/assets/js/13.f58a5ded.js"><link rel="prefetch" href="/assets/js/14.269eb641.js"><link rel="prefetch" href="/assets/js/15.40d2c0cf.js"><link rel="prefetch" href="/assets/js/17.4851e27a.js"><link rel="prefetch" href="/assets/js/18.8b83e144.js"><link rel="prefetch" href="/assets/js/19.c8730111.js"><link rel="prefetch" href="/assets/js/20.19d1cbe1.js"><link rel="prefetch" href="/assets/js/21.a79aa63f.js"><link rel="prefetch" href="/assets/js/22.7b8ded85.js"><link rel="prefetch" href="/assets/js/23.a7b93b05.js"><link rel="prefetch" href="/assets/js/24.a20d4bb7.js"><link rel="prefetch" href="/assets/js/25.4deeaec9.js"><link rel="prefetch" href="/assets/js/26.631fe133.js"><link rel="prefetch" href="/assets/js/27.ba05f0b5.js"><link rel="prefetch" href="/assets/js/28.ea88ddc3.js"><link rel="prefetch" href="/assets/js/29.54076f26.js"><link rel="prefetch" href="/assets/js/3.736cdea2.js"><link rel="prefetch" href="/assets/js/30.1bb315fb.js"><link rel="prefetch" href="/assets/js/31.0ac58d90.js"><link rel="prefetch" href="/assets/js/32.947e4e92.js"><link rel="prefetch" href="/assets/js/33.436c3bb6.js"><link rel="prefetch" href="/assets/js/34.7da1a08f.js"><link rel="prefetch" href="/assets/js/35.e1805606.js"><link rel="prefetch" href="/assets/js/36.14a21cbe.js"><link rel="prefetch" href="/assets/js/37.9d95c874.js"><link rel="prefetch" href="/assets/js/38.ed00ea11.js"><link rel="prefetch" href="/assets/js/39.8c9ed3d7.js"><link rel="prefetch" href="/assets/js/4.99c371bb.js"><link rel="prefetch" href="/assets/js/40.3acf80a1.js"><link rel="prefetch" href="/assets/js/41.68ec6bb9.js"><link rel="prefetch" href="/assets/js/42.8d291817.js"><link rel="prefetch" href="/assets/js/43.04b88cf3.js"><link rel="prefetch" href="/assets/js/44.d3d09b19.js"><link rel="prefetch" href="/assets/js/45.bdc277c6.js"><link rel="prefetch" href="/assets/js/46.1c9a3c14.js"><link rel="prefetch" href="/assets/js/47.475cdb5e.js"><link rel="prefetch" href="/assets/js/48.c5f2409a.js"><link rel="prefetch" href="/assets/js/49.1d13c570.js"><link rel="prefetch" href="/assets/js/5.a24d6838.js"><link rel="prefetch" href="/assets/js/50.b4bea273.js"><link rel="prefetch" href="/assets/js/51.b2425485.js"><link rel="prefetch" href="/assets/js/52.5b562e31.js"><link rel="prefetch" href="/assets/js/53.9db7815d.js"><link rel="prefetch" href="/assets/js/54.ed61c281.js"><link rel="prefetch" href="/assets/js/55.40015417.js"><link rel="prefetch" href="/assets/js/56.d3c74b1f.js"><link rel="prefetch" href="/assets/js/57.2c302e4a.js"><link rel="prefetch" href="/assets/js/58.a20caa6b.js"><link rel="prefetch" href="/assets/js/59.ca9c2a02.js"><link rel="prefetch" href="/assets/js/6.813c1476.js"><link rel="prefetch" href="/assets/js/60.c5c2e28f.js"><link rel="prefetch" href="/assets/js/61.634fe042.js"><link rel="prefetch" href="/assets/js/62.71d03150.js"><link rel="prefetch" href="/assets/js/63.997741ce.js"><link rel="prefetch" href="/assets/js/64.5486b729.js"><link rel="prefetch" href="/assets/js/65.5bf55bed.js"><link rel="prefetch" href="/assets/js/66.f24d51b5.js"><link rel="prefetch" href="/assets/js/67.531749c3.js"><link rel="prefetch" href="/assets/js/68.52e6d9d2.js"><link rel="prefetch" href="/assets/js/69.a2cae568.js"><link rel="prefetch" href="/assets/js/7.b1278ca1.js"><link rel="prefetch" href="/assets/js/70.e524a3a2.js"><link rel="prefetch" href="/assets/js/71.2aac9633.js"><link rel="prefetch" href="/assets/js/72.8e26c8e0.js"><link rel="prefetch" href="/assets/js/73.a9172e83.js"><link rel="prefetch" href="/assets/js/74.e67ca143.js"><link rel="prefetch" href="/assets/js/8.9d3b4527.js"><link rel="prefetch" href="/assets/js/9.f1f6f0e7.js">
    <link rel="stylesheet" href="/assets/css/0.styles.71478383.css">
  </head>
  <body>
    <div id="app" data-server-rendered="true"><div class="theme-container"><header class="navbar"><div class="sidebar-button"><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" role="img" viewBox="0 0 448 512" class="icon"><path fill="currentColor" d="M436 124H12c-6.627 0-12-5.373-12-12V80c0-6.627 5.373-12 12-12h424c6.627 0 12 5.373 12 12v32c0 6.627-5.373 12-12 12zm0 160H12c-6.627 0-12-5.373-12-12v-32c0-6.627 5.373-12 12-12h424c6.627 0 12 5.373 12 12v32c0 6.627-5.373 12-12 12zm0 160H12c-6.627 0-12-5.373-12-12v-32c0-6.627 5.373-12 12-12h424c6.627 0 12 5.373 12 12v32c0 6.627-5.373 12-12 12z"></path></svg></div> <a href="/" class="home-link router-link-active"><!----> <span class="site-name">工具箱的深度学习记事簿</span></a> <div class="links"><div class="search-box"><input aria-label="Search" autocomplete="off" spellcheck="false" value=""> <!----></div> <nav class="nav-links can-hide"><div class="nav-item"><a href="https://github.com/VisualDust/ml.akasaki.space" target="_blank" rel="noopener noreferrer" class="nav-link external">
  View on Github
  <span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a></div><div class="nav-item"><a href="https://github.com/VisualDust" target="_blank" rel="noopener noreferrer" class="nav-link external">
  工具箱
  <span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a></div> <!----></nav></div></header> <div class="sidebar-mask"></div> <aside class="sidebar"><nav class="nav-links"><div class="nav-item"><a href="https://github.com/VisualDust/ml.akasaki.space" target="_blank" rel="noopener noreferrer" class="nav-link external">
  View on Github
  <span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a></div><div class="nav-item"><a href="https://github.com/VisualDust" target="_blank" rel="noopener noreferrer" class="nav-link external">
  工具箱
  <span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a></div> <!----></nav>  <ul class="sidebar-links"><li><section class="sidebar-group collapsable depth-0"><p class="sidebar-heading"><span>第零章：在开始之前</span> <span class="arrow right"></span></p> <!----></section></li><li><section class="sidebar-group collapsable depth-0"><p class="sidebar-heading"><span>第一章上：HelloWorld</span> <span class="arrow right"></span></p> <!----></section></li><li><section class="sidebar-group collapsable depth-0"><p class="sidebar-heading"><span>第一章下：深度学习基础</span> <span class="arrow right"></span></p> <!----></section></li><li><section class="sidebar-group collapsable depth-0"><p class="sidebar-heading"><span>第二章上：卷积神经网络</span> <span class="arrow right"></span></p> <!----></section></li><li><section class="sidebar-group collapsable depth-0"><p class="sidebar-heading open"><span>第二章下：经典卷积神经网络</span> <span class="arrow down"></span></p> <ul class="sidebar-links sidebar-group-items"><li><a href="/ch2p2/[1]LeNet.html" class="sidebar-link">LeNet：初试卷积神经网络</a></li><li><a href="/ch2p2/[2]LeNet-code.html" class="sidebar-link">LeNet代码实现</a></li><li><a href="/ch2p2/[3]write-code-with-keras.html" class="sidebar-link">新玩具：Keras API</a></li><li><a href="/ch2p2/[4]AlexNet.html" class="active sidebar-link">AlexNet：更深的卷积神经网络</a><ul class="sidebar-sub-headers"><li class="sidebar-sub-header"><a href="/ch2p2/[4]AlexNet.html#alexnet模型" class="sidebar-link">AlexNet模型</a></li><li class="sidebar-sub-header"><a href="/ch2p2/[4]AlexNet.html#使用代码实现" class="sidebar-link">使用代码实现</a></li></ul></li><li><a href="/ch2p2/[5]AlexNet-code.html" class="sidebar-link">AlexNet代码实现</a></li><li><a href="/ch2p2/[6]VGGNet.html" class="sidebar-link">VGG：可复用的网络块</a></li><li><a href="/ch2p2/[7]VGGNet-code.html" class="sidebar-link">VGG的代码实现</a></li><li><a href="/ch2p2/[8]GoogLeNet.html" class="sidebar-link">GoogLeNet：致敬LeNet</a></li><li><a href="/ch2p2/[9]GoogLeNet-code.html" class="sidebar-link">GoogLeNet代码实现</a></li><li><a href="/ch2p2/[10]ResNet.html" class="sidebar-link">ResNet：残差网络</a></li><li><a href="/ch2p2/[11]ResNet-code.html" class="sidebar-link">ResNet代码实现</a></li></ul></section></li><li><section class="sidebar-group collapsable depth-0"><p class="sidebar-heading"><span>第三章上：谈一些计算机视觉方向</span> <span class="arrow right"></span></p> <!----></section></li><li><section class="sidebar-group collapsable depth-0"><p class="sidebar-heading"><span>第三章下：一些计算机视觉任务</span> <span class="arrow right"></span></p> <!----></section></li><li><section class="sidebar-group collapsable depth-0"><p class="sidebar-heading"><span>附录：永远是你的好朋友</span> <span class="arrow right"></span></p> <!----></section></li><li><section class="sidebar-group collapsable depth-0"><p class="sidebar-heading"><span>第五章：Playground</span> <span class="arrow right"></span></p> <!----></section></li><li><section class="sidebar-group collapsable depth-0"><p class="sidebar-heading"><span>第-1章：TensorFlow编程策略</span> <span class="arrow right"></span></p> <!----></section></li><li><section class="sidebar-group collapsable depth-0"><p class="sidebar-heading"><span>第-2章：数字信号处理（DSP）</span> <span class="arrow right"></span></p> <!----></section></li><li><section class="sidebar-group collapsable depth-0"><p class="sidebar-heading"><span>魔法部日志（又名论文阅读日志）</span> <span class="arrow right"></span></p> <!----></section></li></ul> </aside> <main class="page"> <div class="theme-default-content content__default"><h1 id="alexnet-更深的卷积神经网络"><a href="#alexnet-更深的卷积神经网络" class="header-anchor">#</a> AlexNet：更深的卷积神经网络</h1> <p>AlexNet是在LeNet的思想基础上将卷积神经网络变得更深的应用。其原论文是<a href="https://papers.nips.cc/paper/2012/file/c399862d3b9d6b76c8436e924a68c45b-Paper.pdf" target="_blank" rel="noopener noreferrer">ImageNet Classification with Deep Convolutional Neural Networks<span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a>。</p> <p>上一节介绍了LeNet-5经典卷积网络模型的构成以及如何实现这样的一个网络，并且在实现的模型上获得了90%以上的正确率，但是LeNet-5缺乏对于更大、更多的图片进行分类的能力(MNIST中图片的分辨率仅为28x28,而通常电子设备捕获的照片至少比这个数值大10倍)。在2012年，Hinton 的学生Alex Krizhevsky借助深度学习的相关理论提出了深度卷积神经网络模型AlexNet。 在2012年的ILSVRC竞赛中(翻阅第1章的深度学习现代应用，那里有更多关于这个大赛的介绍)，AlexNet模型取得了top-5 错误率为15.3%的好成绩，相较于Top-5错误率为16.2%的第二名以明显的优势胜出。从此，AlexNet 成为CNN领域比较有标志性的一个网络模型。</p> <blockquote><p>警告：如果你在使用GPU运算的话，AlexNet可能需要超过4GB的显存来运行。</p></blockquote> <p>相较于LeNet-5，AlexNet算得上是一个进化版本。网络结构的情况是，AlexNet 包含6亿3000万个左右的连接，参数的数量有6000万(60M)左右，神经元单元的数量有大概65万个。卷积层的数量有5个，池化层的数量有3个，也就是说，并不是所有的卷积层后面都连接有池化层。在这些卷积与池化层之后是3个全连层，最后一个全连层的单元数量为1000个，用于完成对ImageNet数据集中的图片完成1000分类(具体分类通过Softmax层实现)。总的来说，AlexNet可以算是神经网络在经历了低谷期之后第一次振聋发聩的发声，运用了深度学习算法的深度神经网络被确立为计算机视觉领域的首选，同时也推动了深度学习在其他领域(如语音识别、自然语言处理等)的发展。</p> <p>根据2012年Alex Krizhevsky 在NIPS ( Conference and Workshop on Neural Information Processing Systems，神经信息处理系统大会)公开发表的论文《ImageNet classification with deep convolutional neural networks》的内容，AlexNet 网络的基本结构如图所示。</p> <p><img src="/assets/img/image-20210501081554855.bc61e17a.png" alt="image-20210501081554855"></p> <p>这是原论文对上图的解说：</p> <blockquote><p>Figure 2: An illustration of the architecture of our CNN, explicitly showing the delineation of responsibilities between the two GPUs. One GPU runs the layer-parts at the top of the figure while the other runs the layer-parts at the bottom. The GPUs communicate only at certain layers. The network’s input is 150,528-dimensional, and the number of neurons in the network’s remaining layers is given by 253,440-186,624-64,896-64,896-43,264-4096-4096-1000.</p></blockquote> <p>在AlexNet提出的时候，正是通用GPU快速发展的一个阶段，AlexNet也不失时机地利用了GPU强大的并行计算能力。在处理神经网络训练过程中出现的大量矩阵运算时，AlexNet使用了两块GPU(NVIDIA的GTX580)进行训练。单个GTX580只有3GB显存，因为有限的显存会限制可训练的网络的最大规模，所以作者将AlexNet分布在2个GPU上，每个GPU的显存只需要存储一半的神经元的参数即可。因为GPU之间通信方便，可以在不通过主机内存的情况下互相访问显存，所以同时使用多块GPU也是非常高效的。另外，AlexNet 的2个子网络并不是在所有的层之间都存在通信，这样设计在降低GPU之间通信的性能损耗方面也做出了贡献。在上图中可以看出，2个GPU处理同一幅图像，并且在每-层的深度都一致，不计入输入层的话AlexNet共有8层，其中前5层是卷积层(包含有两个最大池化层)，后3层是全连层。</p> <p>数据增强在模型的训练和测试过程中起到了一定的帮助作用。在训练时，模型会随机地从256*256大小的原始图像中截取224x224 大小的区域，同时还得到了图像进行水平翻转后的镜像，这相当于增加了样本的数量。</p> <h2 id="alexnet模型"><a href="#alexnet模型" class="header-anchor">#</a> AlexNet模型</h2> <p>接下来，我们看一下AlexNet 网络的一些细节。</p> <p><img src="/assets/img/image-20210501081554855.bc61e17a.png" alt="image-20210501081554855"></p> <p>由于在下一小节中我们的设计是将整个AlexNet放在一块GPU而不是拆分成两个模型放在两块GPU上运行，所以在介绍这些网络细节时，我们也将AlexNet 看作一个完整的网络。第一段卷积(convl)中，AlexNet 使用96个11x11卷积核对输入的224x224大小且深度为3的图像进行滤波操作，步长stride 参数为4x4，得到的结果是96个55*55的特征图;得到基本的卷积数据后，第二个操作是ReLU去线性化;第三个操作是LRN ( AlexNet首次提出) ;第四个操作是3x3的最大池化，步长为2。图8-3展示了第一-段卷积的大概过程。</p> <p><img src="/assets/img/image-20210430214555854.53b53890.png" alt="image-20210430214555854"></p> <p>第二段卷积(conv2)接收了来自convl输出的数据，也包含4个操作。第一个操作是卷积操作，使用256个5x5深度为3的卷积核，步长stride参数为1x1，得到的结果是256个27x27的特征图;得到基本的卷积数据后，第二个操作也是ReLU去线性化;第三个操作也是LRN;第四个操作是3x3的最大池化，步长为2。图8~4展示了第二段卷积的大概过程。</p> <p><img src="/assets/img/image-20210430214643575.c82145f6.png" alt="image-20210430214643575"></p> <p>第三段卷积(conv3) 接收了来自(conv2输出的数据，但是这一层去掉了池化操作和LRN。第一个操作是3x3的卷积操作，核数量为384，步长stride参数为1，得到的结果是384个13x13的特征图;得到基本的卷积数据后，下一个操作是ReLU去线性化。图8-5展示了第三段卷积的大概过程。</p> <p><img src="/assets/img/image-20210430215648935.57b7ab05.png" alt="image-20210430215648935"></p> <p>第四段卷积与第三段卷积的实现类似，第五段卷积在第四段卷积的基础上增加了一个最大池化操作。关于这两段卷积这里不再细说，图8-6将这两段卷积展示在了一起。</p> <p><img src="/assets/img/image-20210430215756653.318254f2.png" alt="image-20210430215756653"></p> <blockquote><p>注意：AlexNet在原论文中被拆分成两个网络并且放到两个GPU上运行训练，起初的AlexNet是直接使用CUDA代码编写的。在本文之后的代码实现中，仅在一台计算机上实现AlexNet。</p></blockquote> <hr> <h2 id="使用代码实现"><a href="#使用代码实现" class="header-anchor">#</a> 使用代码实现</h2> <p>AlexNet的出现本身就带有很多开创性的特点。ReLU激活函数的提出要比AlexNet还早，但是在AlexNet之前，并没有关于CNN使用ReLU激活函数获得重大成功的例子。<strong>一些经典的激活函数，如sigmoid, 会在网络较深时产生梯度弥散的问题</strong>。<strong>AlexNet使用ReLU作为CNN的激活函数取得了成功</strong>，原因就在于<strong>ReLU激活函数在较深的网络中能够有效地克服sigmoid存在的梯度弥散问题</strong>。</p> <p>我们一般会在卷积层之后直接添加一个池化层进行处理，但是AlexNet 在卷积层和池化层之间还加入了一个LRN层。LRN（Local Response Normalization，局部相应归一化）层是在AlexNet中首次被提出并运用。对LRN层的描述最早见于Alex 那篇用CNN参加ImageNet比赛的论文，Alex在论文中的解释是:LRN层为了模仿生物神经系统的“侧抑制”机制而对局部神经元的活动创建竞争环境，<strong>这样做会让其中响应比较大的值变得相对更大，并抑制其他响应较小的神经元，能够进一步增强模型的泛化能力</strong>。随后，Alex 在ImageNet数据集上分别测试了添加LRN层的AlexNet以及没有添加LRN层的AlexNet。在两个网络结构完全相同的情况下，他发现使用了LRN层的CNN可以使top-l错误率有1.4%的降低，可以使top-5错误率有1.2%的降低。</p> <blockquote><p>为了方便，从AlexNet开始，我们大量使用KerasAPI进行快速的代码编写。</p></blockquote> <p>导入所需的包：</p> <div class="language-python line-numbers-mode"><pre class="language-python"><code><span class="token keyword">import</span> tensorflow <span class="token keyword">as</span> tf
<span class="token keyword">from</span> tensorflow<span class="token punctuation">.</span>keras <span class="token keyword">import</span> layers<span class="token punctuation">,</span> datasets<span class="token punctuation">,</span> Sequential
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br></div></div><p>这次我们在<code>cifar10</code>上训练AlexNet。导入<code>cifar10</code>数据集并做一次标准化：</p> <div class="language-python line-numbers-mode"><pre class="language-python"><code><span class="token punctuation">(</span>training_x<span class="token punctuation">,</span> training_y<span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token punctuation">(</span>testing_x<span class="token punctuation">,</span> testing_y<span class="token punctuation">)</span> <span class="token operator">=</span> datasets<span class="token punctuation">.</span>cifar10<span class="token punctuation">.</span>load_data<span class="token punctuation">(</span><span class="token punctuation">)</span>
training_x <span class="token operator">=</span> <span class="token punctuation">(</span>training_x<span class="token punctuation">.</span>astype<span class="token punctuation">(</span><span class="token string">'float32'</span><span class="token punctuation">)</span> <span class="token operator">/</span> <span class="token number">255</span><span class="token punctuation">.</span><span class="token punctuation">)</span>
testing_x <span class="token operator">=</span> <span class="token punctuation">(</span>testing_x<span class="token punctuation">.</span>astype<span class="token punctuation">(</span><span class="token string">'float32'</span><span class="token punctuation">)</span> <span class="token operator">/</span> <span class="token number">255</span><span class="token punctuation">.</span><span class="token punctuation">)</span>
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br></div></div><p>接下来定义模型：</p> <div class="language-python line-numbers-mode"><pre class="language-python"><code>model <span class="token operator">=</span> Sequential<span class="token punctuation">(</span><span class="token punctuation">[</span>
    <span class="token comment"># 第一层卷积</span>
    layers<span class="token punctuation">.</span>Conv2D<span class="token punctuation">(</span>filters<span class="token operator">=</span><span class="token number">96</span><span class="token punctuation">,</span> kernel_size<span class="token operator">=</span><span class="token punctuation">(</span><span class="token number">3</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">,</span>
    <span class="token comment"># 第一层卷积具有BN层</span>
    layers<span class="token punctuation">.</span>BatchNormalization<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span>
    layers<span class="token punctuation">.</span>Activation<span class="token punctuation">(</span><span class="token string">'relu'</span><span class="token punctuation">)</span><span class="token punctuation">,</span>
    layers<span class="token punctuation">.</span>MaxPool2D<span class="token punctuation">(</span>pool_size<span class="token operator">=</span><span class="token punctuation">(</span><span class="token number">3</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">)</span><span class="token punctuation">,</span> strides<span class="token operator">=</span><span class="token number">2</span><span class="token punctuation">)</span><span class="token punctuation">,</span>
    <span class="token comment"># 第二层卷积</span>
    layers<span class="token punctuation">.</span>Conv2D<span class="token punctuation">(</span>filters<span class="token operator">=</span><span class="token number">256</span><span class="token punctuation">,</span> kernel_size<span class="token operator">=</span><span class="token punctuation">(</span><span class="token number">3</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">,</span>
    <span class="token comment"># 第二层卷积具有BN层</span>
    layers<span class="token punctuation">.</span>BatchNormalization<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span>
    layers<span class="token punctuation">.</span>Activation<span class="token punctuation">(</span><span class="token string">'relu'</span><span class="token punctuation">)</span><span class="token punctuation">,</span>
    layers<span class="token punctuation">.</span>MaxPool2D<span class="token punctuation">(</span>pool_size<span class="token operator">=</span><span class="token punctuation">(</span><span class="token number">3</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">)</span><span class="token punctuation">,</span> strides<span class="token operator">=</span><span class="token number">2</span><span class="token punctuation">)</span><span class="token punctuation">,</span>
    <span class="token comment"># 第三层卷积，并没有BN层，也没有池化</span>
    layers<span class="token punctuation">.</span>Conv2D<span class="token punctuation">(</span>filters<span class="token operator">=</span><span class="token number">384</span><span class="token punctuation">,</span> kernel_size<span class="token operator">=</span><span class="token punctuation">(</span><span class="token number">3</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">)</span><span class="token punctuation">,</span> padding<span class="token operator">=</span><span class="token string">'same'</span><span class="token punctuation">,</span>
                  activation<span class="token operator">=</span><span class="token string">'relu'</span><span class="token punctuation">)</span><span class="token punctuation">,</span>
    <span class="token comment"># 第四层卷积，并没有BN层，也没有池化</span>
    layers<span class="token punctuation">.</span>Conv2D<span class="token punctuation">(</span>filters<span class="token operator">=</span><span class="token number">384</span><span class="token punctuation">,</span> kernel_size<span class="token operator">=</span><span class="token punctuation">(</span><span class="token number">3</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">)</span><span class="token punctuation">,</span> padding<span class="token operator">=</span><span class="token string">'same'</span><span class="token punctuation">,</span>
                  activation<span class="token operator">=</span><span class="token string">'relu'</span><span class="token punctuation">)</span><span class="token punctuation">,</span>
    <span class="token comment"># 第五层卷积，并没有BN层</span>
    layers<span class="token punctuation">.</span>Conv2D<span class="token punctuation">(</span>filters<span class="token operator">=</span><span class="token number">256</span><span class="token punctuation">,</span> kernel_size<span class="token operator">=</span><span class="token punctuation">(</span><span class="token number">3</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">)</span><span class="token punctuation">,</span> padding<span class="token operator">=</span><span class="token string">'same'</span><span class="token punctuation">,</span>
                  activation<span class="token operator">=</span><span class="token string">'relu'</span><span class="token punctuation">)</span><span class="token punctuation">,</span>
    layers<span class="token punctuation">.</span>MaxPool2D<span class="token punctuation">(</span>pool_size<span class="token operator">=</span><span class="token punctuation">(</span><span class="token number">3</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">)</span><span class="token punctuation">,</span> strides<span class="token operator">=</span><span class="token number">2</span><span class="token punctuation">)</span><span class="token punctuation">,</span>
    <span class="token comment"># 打平进入全连接进行分类</span>
    layers<span class="token punctuation">.</span>Flatten<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span>
    layers<span class="token punctuation">.</span>Dense<span class="token punctuation">(</span><span class="token number">2048</span><span class="token punctuation">,</span> activation<span class="token operator">=</span><span class="token string">'relu'</span><span class="token punctuation">)</span><span class="token punctuation">,</span>
    layers<span class="token punctuation">.</span>Dropout<span class="token punctuation">(</span><span class="token number">0.5</span><span class="token punctuation">)</span><span class="token punctuation">,</span>
    layers<span class="token punctuation">.</span>Dense<span class="token punctuation">(</span><span class="token number">2048</span><span class="token punctuation">,</span> activation<span class="token operator">=</span><span class="token string">'relu'</span><span class="token punctuation">)</span><span class="token punctuation">,</span>
    layers<span class="token punctuation">.</span>Dropout<span class="token punctuation">(</span><span class="token number">0.5</span><span class="token punctuation">)</span><span class="token punctuation">,</span>
    layers<span class="token punctuation">.</span>Dense<span class="token punctuation">(</span><span class="token number">10</span><span class="token punctuation">,</span> activation<span class="token operator">=</span><span class="token string">'softmax'</span><span class="token punctuation">)</span>
<span class="token punctuation">]</span><span class="token punctuation">)</span>
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br><span class="line-number">7</span><br><span class="line-number">8</span><br><span class="line-number">9</span><br><span class="line-number">10</span><br><span class="line-number">11</span><br><span class="line-number">12</span><br><span class="line-number">13</span><br><span class="line-number">14</span><br><span class="line-number">15</span><br><span class="line-number">16</span><br><span class="line-number">17</span><br><span class="line-number">18</span><br><span class="line-number">19</span><br><span class="line-number">20</span><br><span class="line-number">21</span><br><span class="line-number">22</span><br><span class="line-number">23</span><br><span class="line-number">24</span><br><span class="line-number">25</span><br><span class="line-number">26</span><br><span class="line-number">27</span><br><span class="line-number">28</span><br><span class="line-number">29</span><br><span class="line-number">30</span><br><span class="line-number">31</span><br></div></div><p>接下来对模型进行训练：</p> <div class="language-python line-numbers-mode"><pre class="language-python"><code>model<span class="token punctuation">.</span><span class="token builtin">compile</span><span class="token punctuation">(</span>optimizer<span class="token operator">=</span><span class="token string">'adam'</span><span class="token punctuation">,</span>
              loss<span class="token operator">=</span>tf<span class="token punctuation">.</span>keras<span class="token punctuation">.</span>losses<span class="token punctuation">.</span>SparseCategoricalCrossentropy<span class="token punctuation">(</span>from_logits<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">)</span><span class="token punctuation">,</span>
              metrics<span class="token operator">=</span><span class="token punctuation">[</span><span class="token string">'sparse_categorical_accuracy'</span><span class="token punctuation">]</span><span class="token punctuation">)</span>

model<span class="token punctuation">.</span>fit<span class="token punctuation">(</span>training_x<span class="token punctuation">,</span> training_y<span class="token punctuation">,</span> batch_size<span class="token operator">=</span><span class="token number">32</span><span class="token punctuation">,</span> epochs<span class="token operator">=</span><span class="token number">5</span><span class="token punctuation">,</span> validation_data<span class="token operator">=</span><span class="token punctuation">(</span>testing_x<span class="token punctuation">,</span> testing_y<span class="token punctuation">)</span><span class="token punctuation">)</span>
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br></div></div><p>与<a href="/ch2p2/[1]LeNet.html">之前的LeNet</a>不同的是，AlexNet在训练的时候使用<code>Cifar10</code>数据集，图像较大，所以这里将batch size设置为32而不是LeNet中的上百。</p> <p>训练输出：</p> <div class="language- line-numbers-mode"><pre class="language-text"><code>Epoch 1/5
1563/1563 [==============================] - 89s 57ms/step - loss: 1.6133 - sparse_categorical_accuracy: 0.4115 - val_loss: 1.5746 - val_sparse_categorical_accuracy: 0.4414
Epoch 2/5
1563/1563 [==============================] - 82s 52ms/step - loss: 1.2800 - sparse_categorical_accuracy: 0.5475 - val_loss: 1.2506 - val_sparse_categorical_accuracy: 0.5565
Epoch 3/5
1563/1563 [==============================] - 82s 52ms/step - loss: 1.1405 - sparse_categorical_accuracy: 0.5994 - val_loss: 1.4479 - val_sparse_categorical_accuracy: 0.4983
Epoch 4/5
1563/1563 [==============================] - 83s 53ms/step - loss: 1.0457 - sparse_categorical_accuracy: 0.6391 - val_loss: 1.2161 - val_sparse_categorical_accuracy: 0.5892
Epoch 5/5
1563/1563 [==============================] - 83s 53ms/step - loss: 0.9849 - sparse_categorical_accuracy: 0.6629 - val_loss: 1.1617 - val_sparse_categorical_accuracy: 0.6053
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br><span class="line-number">7</span><br><span class="line-number">8</span><br><span class="line-number">9</span><br><span class="line-number">10</span><br></div></div><p>由于我在写作本文的时候使用一台办公本进行了训练，所以整个训练过程是缓慢的，我只训练了五个epochs就停了下来。但是能观察到，精确度正在逐渐上升。</p></div> <footer class="page-edit"><!----> <div class="last-updated"><span class="prefix">上次更新时间:</span> <span class="time">5/11/2021, 6:22:54 PM</span></div></footer> <div class="page-nav"><p class="inner"><span class="prev">
      ←
      <a href="/ch2p2/[3]write-code-with-keras.html" class="prev">
        新玩具：Keras API
      </a></span> <span class="next"><a href="/ch2p2/[5]AlexNet-code.html">
        AlexNet代码实现
      </a>
      →
    </span></p></div> </main></div><div class="global-ui"></div></div>
    <script src="/assets/js/app.579bc41f.js" defer></script><script src="/assets/js/2.024d806c.js" defer></script><script src="/assets/js/16.882eb06c.js" defer></script>
  </body>
</html>
